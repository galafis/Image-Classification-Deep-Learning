{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9K94P43WQGB"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d prasunroy/natural-images"
      ],
      "metadata": {
        "id": "vGPSxuqdWkWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7306825c-4c39-4fbf-e0a0-2e07129d6919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading natural-images.zip to /content\n",
            " 99% 340M/342M [00:17<00:00, 22.4MB/s]\n",
            "100% 342M/342M [00:17<00:00, 20.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip natural-images.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WI5xiX2W3Ke",
        "outputId": "3400f298-f12b-4055-fbdd-b14922a1e93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  natural-images.zip\n",
            "replace data/natural_images/airplane/airplane_0000.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.utils import make_grid\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "weZwFvOGW-dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "device = get_default_device()\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAsKy95RQcvN",
        "outputId": "04d8aab9-98dd-48bb-e81b-6de99a05b186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = os.listdir('./natural_images')\n",
        "num_classes = len(classes)\n",
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHmCCrmlXB9k",
        "outputId": "9c68408c-28ae-4d8b-a07b-2120769e6a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "        # Randomly augment the image data\n",
        "            # Random horizontal flip\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "            # Random vertical flip\n",
        "        transforms.RandomVerticalFlip(0.3),\n",
        "        # transform to tensors\n",
        "        transforms.ToTensor(),\n",
        "        # Normalize the pixel values (in R, G, and B channels)\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])"
      ],
      "metadata": {
        "id": "QcXcPvHSXIQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "\n",
        "all_data = datasets.ImageFolder(root='./natural_images')\n",
        "train_data_len = int(len(all_data)*0.7)\n",
        "test_data_len = int(len(all_data) - train_data_len)\n",
        "train_data, test_data = random_split(all_data, [train_data_len, test_data_len])\n",
        "train_data.dataset.transform = transform\n",
        "test_data.dataset.transform = transform\n",
        "print(len(train_data),len(test_data))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkMmud1bYb3c",
        "outputId": "5842e645-0c68-4489-e0e8-d5634f8c9998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4829 2070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the spatial size of images\n",
        "sample_image, _ = train_data[0]  # Retrieve the first image sample\n",
        "image_size = sample_image.shape[1:]   # Get the size of the image\n",
        "\n",
        "print(\"Spatial size of images: \", image_size)\n",
        "num_channels = sample_image.shape[0]\n",
        "\n",
        "print(\"Number of channels:\", num_channels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REKqZVRbc84k",
        "outputId": "1f06ae39-eaf7-4785-ddf6-5fd54d85c499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spatial size of images:  torch.Size([256, 256])\n",
            "Number of channels: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 12, 3, 1, 1)\n",
        "    self.conv2 = nn.Conv2d(12, 24, 3, 1, 1)\n",
        "    self.conv3 = nn.Conv2d(24, 48, 3, 1, 1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.drop = nn.Dropout2d(0.2)\n",
        "    self.fc = nn.Linear(32*32*48, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.pool(self.conv1(x)))\n",
        "    x = F.relu(self.pool(self.conv2(x)))\n",
        "    x = F.relu(self.pool(self.conv3(x)))\n",
        "    x = F.dropout(self.drop(x), training=self.training)\n",
        "    x = x.view(-1, 32*32*48)\n",
        "    x = self.fc(x)\n",
        "    return torch.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "u6V6NzTYZAMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model = model.to(device)\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "7OAZpQgWfbzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7746da9f-cc68-4f94-c63f-9dc5886ec208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (drop): Dropout2d(p=0.2, inplace=False)\n",
              "  (fc): Linear(in_features=49152, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# Specify the loss criteria\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "76LB5VOrgCj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs for training\n",
        "num_epochs = 10\n",
        "total_step = len(train_loader)\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    print('Epoch finished')\n",
        "\n",
        "print('Training finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ4XOQbwgSe7",
        "outputId": "cda5ba21-44de-4409-f0f7-4e8348415f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [1/97], Loss: 2.0812\n",
            "Epoch [1/10], Step [2/97], Loss: 2.0555\n",
            "Epoch [1/10], Step [3/97], Loss: 1.9362\n",
            "Epoch [1/10], Step [4/97], Loss: 1.9079\n",
            "Epoch [1/10], Step [5/97], Loss: 1.8729\n",
            "Epoch [1/10], Step [6/97], Loss: 1.7775\n",
            "Epoch [1/10], Step [7/97], Loss: 1.6285\n",
            "Epoch [1/10], Step [8/97], Loss: 1.3489\n",
            "Epoch [1/10], Step [9/97], Loss: 1.4812\n",
            "Epoch [1/10], Step [10/97], Loss: 1.2851\n",
            "Epoch [1/10], Step [11/97], Loss: 1.9442\n",
            "Epoch [1/10], Step [12/97], Loss: 1.0734\n",
            "Epoch [1/10], Step [13/97], Loss: 1.1978\n",
            "Epoch [1/10], Step [14/97], Loss: 1.4695\n",
            "Epoch [1/10], Step [15/97], Loss: 1.2128\n",
            "Epoch [1/10], Step [16/97], Loss: 1.1296\n",
            "Epoch [1/10], Step [17/97], Loss: 1.4060\n",
            "Epoch [1/10], Step [18/97], Loss: 1.1174\n",
            "Epoch [1/10], Step [19/97], Loss: 1.1718\n",
            "Epoch [1/10], Step [20/97], Loss: 1.1002\n",
            "Epoch [1/10], Step [21/97], Loss: 1.4183\n",
            "Epoch [1/10], Step [22/97], Loss: 0.9467\n",
            "Epoch [1/10], Step [23/97], Loss: 0.9739\n",
            "Epoch [1/10], Step [24/97], Loss: 0.9558\n",
            "Epoch [1/10], Step [25/97], Loss: 0.9129\n",
            "Epoch [1/10], Step [26/97], Loss: 1.2800\n",
            "Epoch [1/10], Step [27/97], Loss: 0.7108\n",
            "Epoch [1/10], Step [28/97], Loss: 1.2858\n",
            "Epoch [1/10], Step [29/97], Loss: 1.2780\n",
            "Epoch [1/10], Step [30/97], Loss: 1.1606\n",
            "Epoch [1/10], Step [31/97], Loss: 0.9074\n",
            "Epoch [1/10], Step [32/97], Loss: 1.0323\n",
            "Epoch [1/10], Step [33/97], Loss: 1.1129\n",
            "Epoch [1/10], Step [34/97], Loss: 0.7601\n",
            "Epoch [1/10], Step [35/97], Loss: 0.9134\n",
            "Epoch [1/10], Step [36/97], Loss: 0.8821\n",
            "Epoch [1/10], Step [37/97], Loss: 0.6869\n",
            "Epoch [1/10], Step [38/97], Loss: 0.8898\n",
            "Epoch [1/10], Step [39/97], Loss: 0.9344\n",
            "Epoch [1/10], Step [40/97], Loss: 0.8378\n",
            "Epoch [1/10], Step [41/97], Loss: 0.8074\n",
            "Epoch [1/10], Step [42/97], Loss: 0.6684\n",
            "Epoch [1/10], Step [43/97], Loss: 0.9247\n",
            "Epoch [1/10], Step [44/97], Loss: 0.7067\n",
            "Epoch [1/10], Step [45/97], Loss: 0.8205\n",
            "Epoch [1/10], Step [46/97], Loss: 0.7599\n",
            "Epoch [1/10], Step [47/97], Loss: 0.7974\n",
            "Epoch [1/10], Step [48/97], Loss: 0.7713\n",
            "Epoch [1/10], Step [49/97], Loss: 0.6939\n",
            "Epoch [1/10], Step [50/97], Loss: 0.5908\n",
            "Epoch [1/10], Step [51/97], Loss: 0.7110\n",
            "Epoch [1/10], Step [52/97], Loss: 0.6728\n",
            "Epoch [1/10], Step [53/97], Loss: 0.7066\n",
            "Epoch [1/10], Step [54/97], Loss: 0.6236\n",
            "Epoch [1/10], Step [55/97], Loss: 0.6045\n",
            "Epoch [1/10], Step [56/97], Loss: 0.5698\n",
            "Epoch [1/10], Step [57/97], Loss: 0.7697\n",
            "Epoch [1/10], Step [58/97], Loss: 0.8565\n",
            "Epoch [1/10], Step [59/97], Loss: 0.5475\n",
            "Epoch [1/10], Step [60/97], Loss: 0.7602\n",
            "Epoch [1/10], Step [61/97], Loss: 0.7802\n",
            "Epoch [1/10], Step [62/97], Loss: 0.6378\n",
            "Epoch [1/10], Step [63/97], Loss: 0.3847\n",
            "Epoch [1/10], Step [64/97], Loss: 0.6295\n",
            "Epoch [1/10], Step [65/97], Loss: 0.8115\n",
            "Epoch [1/10], Step [66/97], Loss: 0.5655\n",
            "Epoch [1/10], Step [67/97], Loss: 0.8174\n",
            "Epoch [1/10], Step [68/97], Loss: 0.5832\n",
            "Epoch [1/10], Step [69/97], Loss: 0.7579\n",
            "Epoch [1/10], Step [70/97], Loss: 0.6494\n",
            "Epoch [1/10], Step [71/97], Loss: 0.7977\n",
            "Epoch [1/10], Step [72/97], Loss: 0.7056\n",
            "Epoch [1/10], Step [73/97], Loss: 0.9207\n",
            "Epoch [1/10], Step [74/97], Loss: 0.8146\n",
            "Epoch [1/10], Step [75/97], Loss: 0.8350\n",
            "Epoch [1/10], Step [76/97], Loss: 0.6915\n",
            "Epoch [1/10], Step [77/97], Loss: 0.6803\n",
            "Epoch [1/10], Step [78/97], Loss: 1.1754\n",
            "Epoch [1/10], Step [79/97], Loss: 0.8711\n",
            "Epoch [1/10], Step [80/97], Loss: 0.5033\n",
            "Epoch [1/10], Step [81/97], Loss: 0.7284\n",
            "Epoch [1/10], Step [82/97], Loss: 0.6844\n",
            "Epoch [1/10], Step [83/97], Loss: 0.6659\n",
            "Epoch [1/10], Step [84/97], Loss: 0.6423\n",
            "Epoch [1/10], Step [85/97], Loss: 0.7556\n",
            "Epoch [1/10], Step [86/97], Loss: 0.8231\n",
            "Epoch [1/10], Step [87/97], Loss: 0.6471\n",
            "Epoch [1/10], Step [88/97], Loss: 0.5214\n",
            "Epoch [1/10], Step [89/97], Loss: 0.6748\n",
            "Epoch [1/10], Step [90/97], Loss: 0.8748\n",
            "Epoch [1/10], Step [91/97], Loss: 0.4837\n",
            "Epoch [1/10], Step [92/97], Loss: 0.7770\n",
            "Epoch [1/10], Step [93/97], Loss: 0.7673\n",
            "Epoch [1/10], Step [94/97], Loss: 0.5677\n",
            "Epoch [1/10], Step [95/97], Loss: 0.5305\n",
            "Epoch [1/10], Step [96/97], Loss: 0.6950\n",
            "Epoch [1/10], Step [97/97], Loss: 0.6909\n",
            "Epoch finished\n",
            "Epoch [2/10], Step [1/97], Loss: 0.6534\n",
            "Epoch [2/10], Step [2/97], Loss: 0.6147\n",
            "Epoch [2/10], Step [3/97], Loss: 0.6991\n",
            "Epoch [2/10], Step [4/97], Loss: 0.5937\n",
            "Epoch [2/10], Step [5/97], Loss: 0.6753\n",
            "Epoch [2/10], Step [6/97], Loss: 0.6184\n",
            "Epoch [2/10], Step [7/97], Loss: 0.6604\n",
            "Epoch [2/10], Step [8/97], Loss: 0.3614\n",
            "Epoch [2/10], Step [9/97], Loss: 0.4757\n",
            "Epoch [2/10], Step [10/97], Loss: 0.2583\n",
            "Epoch [2/10], Step [11/97], Loss: 1.3176\n",
            "Epoch [2/10], Step [12/97], Loss: 0.4211\n",
            "Epoch [2/10], Step [13/97], Loss: 0.7809\n",
            "Epoch [2/10], Step [14/97], Loss: 0.4740\n",
            "Epoch [2/10], Step [15/97], Loss: 0.6387\n",
            "Epoch [2/10], Step [16/97], Loss: 0.5452\n",
            "Epoch [2/10], Step [17/97], Loss: 0.6167\n",
            "Epoch [2/10], Step [18/97], Loss: 0.4459\n",
            "Epoch [2/10], Step [19/97], Loss: 0.4618\n",
            "Epoch [2/10], Step [20/97], Loss: 0.5146\n",
            "Epoch [2/10], Step [21/97], Loss: 0.6701\n",
            "Epoch [2/10], Step [22/97], Loss: 0.5417\n",
            "Epoch [2/10], Step [23/97], Loss: 0.3517\n",
            "Epoch [2/10], Step [24/97], Loss: 0.5697\n",
            "Epoch [2/10], Step [25/97], Loss: 0.3644\n",
            "Epoch [2/10], Step [26/97], Loss: 0.7461\n",
            "Epoch [2/10], Step [27/97], Loss: 0.3032\n",
            "Epoch [2/10], Step [28/97], Loss: 0.5804\n",
            "Epoch [2/10], Step [29/97], Loss: 0.5405\n",
            "Epoch [2/10], Step [30/97], Loss: 0.6957\n",
            "Epoch [2/10], Step [31/97], Loss: 0.4789\n",
            "Epoch [2/10], Step [32/97], Loss: 0.3477\n",
            "Epoch [2/10], Step [33/97], Loss: 0.4981\n",
            "Epoch [2/10], Step [34/97], Loss: 0.4577\n",
            "Epoch [2/10], Step [35/97], Loss: 0.4985\n",
            "Epoch [2/10], Step [36/97], Loss: 0.5994\n",
            "Epoch [2/10], Step [37/97], Loss: 0.3652\n",
            "Epoch [2/10], Step [38/97], Loss: 0.4236\n",
            "Epoch [2/10], Step [39/97], Loss: 0.4317\n",
            "Epoch [2/10], Step [40/97], Loss: 0.6050\n",
            "Epoch [2/10], Step [41/97], Loss: 0.6985\n",
            "Epoch [2/10], Step [42/97], Loss: 0.3705\n",
            "Epoch [2/10], Step [43/97], Loss: 0.6173\n",
            "Epoch [2/10], Step [44/97], Loss: 0.3055\n",
            "Epoch [2/10], Step [45/97], Loss: 0.5216\n",
            "Epoch [2/10], Step [46/97], Loss: 0.3664\n",
            "Epoch [2/10], Step [47/97], Loss: 0.3504\n",
            "Epoch [2/10], Step [48/97], Loss: 0.4898\n",
            "Epoch [2/10], Step [49/97], Loss: 0.4452\n",
            "Epoch [2/10], Step [50/97], Loss: 0.4482\n",
            "Epoch [2/10], Step [51/97], Loss: 0.4536\n",
            "Epoch [2/10], Step [52/97], Loss: 0.4608\n",
            "Epoch [2/10], Step [53/97], Loss: 0.5643\n",
            "Epoch [2/10], Step [54/97], Loss: 0.3944\n",
            "Epoch [2/10], Step [55/97], Loss: 0.3826\n",
            "Epoch [2/10], Step [56/97], Loss: 0.2568\n",
            "Epoch [2/10], Step [57/97], Loss: 0.4437\n",
            "Epoch [2/10], Step [58/97], Loss: 0.6541\n",
            "Epoch [2/10], Step [59/97], Loss: 0.3823\n",
            "Epoch [2/10], Step [60/97], Loss: 0.4777\n",
            "Epoch [2/10], Step [61/97], Loss: 0.5162\n",
            "Epoch [2/10], Step [62/97], Loss: 0.3528\n",
            "Epoch [2/10], Step [63/97], Loss: 0.2578\n",
            "Epoch [2/10], Step [64/97], Loss: 0.4687\n",
            "Epoch [2/10], Step [65/97], Loss: 0.7499\n",
            "Epoch [2/10], Step [66/97], Loss: 0.4987\n",
            "Epoch [2/10], Step [67/97], Loss: 0.5208\n",
            "Epoch [2/10], Step [68/97], Loss: 0.3932\n",
            "Epoch [2/10], Step [69/97], Loss: 0.6241\n",
            "Epoch [2/10], Step [70/97], Loss: 0.3534\n",
            "Epoch [2/10], Step [71/97], Loss: 0.3932\n",
            "Epoch [2/10], Step [72/97], Loss: 0.5466\n",
            "Epoch [2/10], Step [73/97], Loss: 0.7470\n",
            "Epoch [2/10], Step [74/97], Loss: 0.5383\n",
            "Epoch [2/10], Step [75/97], Loss: 0.4869\n",
            "Epoch [2/10], Step [76/97], Loss: 0.3631\n",
            "Epoch [2/10], Step [77/97], Loss: 0.5669\n",
            "Epoch [2/10], Step [78/97], Loss: 0.6180\n",
            "Epoch [2/10], Step [79/97], Loss: 0.4975\n",
            "Epoch [2/10], Step [80/97], Loss: 0.3451\n",
            "Epoch [2/10], Step [81/97], Loss: 0.5326\n",
            "Epoch [2/10], Step [82/97], Loss: 0.4609\n",
            "Epoch [2/10], Step [83/97], Loss: 0.6024\n",
            "Epoch [2/10], Step [84/97], Loss: 0.2905\n",
            "Epoch [2/10], Step [85/97], Loss: 0.3729\n",
            "Epoch [2/10], Step [86/97], Loss: 0.7633\n",
            "Epoch [2/10], Step [87/97], Loss: 0.4028\n",
            "Epoch [2/10], Step [88/97], Loss: 0.3196\n",
            "Epoch [2/10], Step [89/97], Loss: 0.4359\n",
            "Epoch [2/10], Step [90/97], Loss: 0.5884\n",
            "Epoch [2/10], Step [91/97], Loss: 0.4413\n",
            "Epoch [2/10], Step [92/97], Loss: 0.4607\n",
            "Epoch [2/10], Step [93/97], Loss: 0.5666\n",
            "Epoch [2/10], Step [94/97], Loss: 0.3716\n",
            "Epoch [2/10], Step [95/97], Loss: 0.3886\n",
            "Epoch [2/10], Step [96/97], Loss: 0.5699\n",
            "Epoch [2/10], Step [97/97], Loss: 0.4495\n",
            "Epoch finished\n",
            "Epoch [3/10], Step [1/97], Loss: 0.5555\n",
            "Epoch [3/10], Step [2/97], Loss: 0.3526\n",
            "Epoch [3/10], Step [3/97], Loss: 0.4700\n",
            "Epoch [3/10], Step [4/97], Loss: 0.4104\n",
            "Epoch [3/10], Step [5/97], Loss: 0.4042\n",
            "Epoch [3/10], Step [6/97], Loss: 0.4151\n",
            "Epoch [3/10], Step [7/97], Loss: 0.3549\n",
            "Epoch [3/10], Step [8/97], Loss: 0.2601\n",
            "Epoch [3/10], Step [9/97], Loss: 0.3112\n",
            "Epoch [3/10], Step [10/97], Loss: 0.3157\n",
            "Epoch [3/10], Step [11/97], Loss: 0.8609\n",
            "Epoch [3/10], Step [12/97], Loss: 0.4798\n",
            "Epoch [3/10], Step [13/97], Loss: 0.5736\n",
            "Epoch [3/10], Step [14/97], Loss: 0.4105\n",
            "Epoch [3/10], Step [15/97], Loss: 0.4441\n",
            "Epoch [3/10], Step [16/97], Loss: 0.3252\n",
            "Epoch [3/10], Step [17/97], Loss: 0.4664\n",
            "Epoch [3/10], Step [18/97], Loss: 0.4709\n",
            "Epoch [3/10], Step [19/97], Loss: 0.3928\n",
            "Epoch [3/10], Step [20/97], Loss: 0.3101\n",
            "Epoch [3/10], Step [21/97], Loss: 0.4590\n",
            "Epoch [3/10], Step [22/97], Loss: 0.3907\n",
            "Epoch [3/10], Step [23/97], Loss: 0.2339\n",
            "Epoch [3/10], Step [24/97], Loss: 0.3723\n",
            "Epoch [3/10], Step [25/97], Loss: 0.3255\n",
            "Epoch [3/10], Step [26/97], Loss: 0.7576\n",
            "Epoch [3/10], Step [27/97], Loss: 0.1824\n",
            "Epoch [3/10], Step [28/97], Loss: 0.4325\n",
            "Epoch [3/10], Step [29/97], Loss: 0.5412\n",
            "Epoch [3/10], Step [30/97], Loss: 0.6842\n",
            "Epoch [3/10], Step [31/97], Loss: 0.3168\n",
            "Epoch [3/10], Step [32/97], Loss: 0.4449\n",
            "Epoch [3/10], Step [33/97], Loss: 0.3803\n",
            "Epoch [3/10], Step [34/97], Loss: 0.3308\n",
            "Epoch [3/10], Step [35/97], Loss: 0.5048\n",
            "Epoch [3/10], Step [36/97], Loss: 0.5625\n",
            "Epoch [3/10], Step [37/97], Loss: 0.2825\n",
            "Epoch [3/10], Step [38/97], Loss: 0.2691\n",
            "Epoch [3/10], Step [39/97], Loss: 0.4541\n",
            "Epoch [3/10], Step [40/97], Loss: 0.2730\n",
            "Epoch [3/10], Step [41/97], Loss: 0.4403\n",
            "Epoch [3/10], Step [42/97], Loss: 0.3036\n",
            "Epoch [3/10], Step [43/97], Loss: 0.5171\n",
            "Epoch [3/10], Step [44/97], Loss: 0.1903\n",
            "Epoch [3/10], Step [45/97], Loss: 0.3570\n",
            "Epoch [3/10], Step [46/97], Loss: 0.3248\n",
            "Epoch [3/10], Step [47/97], Loss: 0.3153\n",
            "Epoch [3/10], Step [48/97], Loss: 0.3979\n",
            "Epoch [3/10], Step [49/97], Loss: 0.2599\n",
            "Epoch [3/10], Step [50/97], Loss: 0.3688\n",
            "Epoch [3/10], Step [51/97], Loss: 0.3845\n",
            "Epoch [3/10], Step [52/97], Loss: 0.3841\n",
            "Epoch [3/10], Step [53/97], Loss: 0.3903\n",
            "Epoch [3/10], Step [54/97], Loss: 0.3464\n",
            "Epoch [3/10], Step [55/97], Loss: 0.4007\n",
            "Epoch [3/10], Step [56/97], Loss: 0.2519\n",
            "Epoch [3/10], Step [57/97], Loss: 0.4845\n",
            "Epoch [3/10], Step [58/97], Loss: 0.4884\n",
            "Epoch [3/10], Step [59/97], Loss: 0.4263\n",
            "Epoch [3/10], Step [60/97], Loss: 0.3996\n",
            "Epoch [3/10], Step [61/97], Loss: 0.4153\n",
            "Epoch [3/10], Step [62/97], Loss: 0.2711\n",
            "Epoch [3/10], Step [63/97], Loss: 0.2915\n",
            "Epoch [3/10], Step [64/97], Loss: 0.3685\n",
            "Epoch [3/10], Step [65/97], Loss: 0.6298\n",
            "Epoch [3/10], Step [66/97], Loss: 0.3043\n",
            "Epoch [3/10], Step [67/97], Loss: 0.5945\n",
            "Epoch [3/10], Step [68/97], Loss: 0.3870\n",
            "Epoch [3/10], Step [69/97], Loss: 0.4181\n",
            "Epoch [3/10], Step [70/97], Loss: 0.2823\n",
            "Epoch [3/10], Step [71/97], Loss: 0.4970\n",
            "Epoch [3/10], Step [72/97], Loss: 0.4308\n",
            "Epoch [3/10], Step [73/97], Loss: 0.7074\n",
            "Epoch [3/10], Step [74/97], Loss: 0.3935\n",
            "Epoch [3/10], Step [75/97], Loss: 0.4896\n",
            "Epoch [3/10], Step [76/97], Loss: 0.2395\n",
            "Epoch [3/10], Step [77/97], Loss: 0.5575\n",
            "Epoch [3/10], Step [78/97], Loss: 0.5542\n",
            "Epoch [3/10], Step [79/97], Loss: 0.3354\n",
            "Epoch [3/10], Step [80/97], Loss: 0.4242\n",
            "Epoch [3/10], Step [81/97], Loss: 0.4156\n",
            "Epoch [3/10], Step [82/97], Loss: 0.3361\n",
            "Epoch [3/10], Step [83/97], Loss: 0.4062\n",
            "Epoch [3/10], Step [84/97], Loss: 0.2972\n",
            "Epoch [3/10], Step [85/97], Loss: 0.4313\n",
            "Epoch [3/10], Step [86/97], Loss: 0.6424\n",
            "Epoch [3/10], Step [87/97], Loss: 0.3281\n",
            "Epoch [3/10], Step [88/97], Loss: 0.2736\n",
            "Epoch [3/10], Step [89/97], Loss: 0.2848\n",
            "Epoch [3/10], Step [90/97], Loss: 0.4982\n",
            "Epoch [3/10], Step [91/97], Loss: 0.3215\n",
            "Epoch [3/10], Step [92/97], Loss: 0.4158\n",
            "Epoch [3/10], Step [93/97], Loss: 0.4885\n",
            "Epoch [3/10], Step [94/97], Loss: 0.4148\n",
            "Epoch [3/10], Step [95/97], Loss: 0.2741\n",
            "Epoch [3/10], Step [96/97], Loss: 0.4234\n",
            "Epoch [3/10], Step [97/97], Loss: 0.2157\n",
            "Epoch finished\n",
            "Epoch [4/10], Step [1/97], Loss: 0.3401\n",
            "Epoch [4/10], Step [2/97], Loss: 0.3552\n",
            "Epoch [4/10], Step [3/97], Loss: 0.4083\n",
            "Epoch [4/10], Step [4/97], Loss: 0.3274\n",
            "Epoch [4/10], Step [5/97], Loss: 0.3886\n",
            "Epoch [4/10], Step [6/97], Loss: 0.3509\n",
            "Epoch [4/10], Step [7/97], Loss: 0.4715\n",
            "Epoch [4/10], Step [8/97], Loss: 0.3156\n",
            "Epoch [4/10], Step [9/97], Loss: 0.3133\n",
            "Epoch [4/10], Step [10/97], Loss: 0.1923\n",
            "Epoch [4/10], Step [11/97], Loss: 0.5390\n",
            "Epoch [4/10], Step [12/97], Loss: 0.2419\n",
            "Epoch [4/10], Step [13/97], Loss: 0.5046\n",
            "Epoch [4/10], Step [14/97], Loss: 0.2893\n",
            "Epoch [4/10], Step [15/97], Loss: 0.4666\n",
            "Epoch [4/10], Step [16/97], Loss: 0.2453\n",
            "Epoch [4/10], Step [17/97], Loss: 0.3697\n",
            "Epoch [4/10], Step [18/97], Loss: 0.4143\n",
            "Epoch [4/10], Step [19/97], Loss: 0.3228\n",
            "Epoch [4/10], Step [20/97], Loss: 0.3391\n",
            "Epoch [4/10], Step [21/97], Loss: 0.5200\n",
            "Epoch [4/10], Step [22/97], Loss: 0.4335\n",
            "Epoch [4/10], Step [23/97], Loss: 0.0994\n",
            "Epoch [4/10], Step [24/97], Loss: 0.2741\n",
            "Epoch [4/10], Step [25/97], Loss: 0.1631\n",
            "Epoch [4/10], Step [26/97], Loss: 0.4462\n",
            "Epoch [4/10], Step [27/97], Loss: 0.1132\n",
            "Epoch [4/10], Step [28/97], Loss: 0.4580\n",
            "Epoch [4/10], Step [29/97], Loss: 0.4033\n",
            "Epoch [4/10], Step [30/97], Loss: 0.4157\n",
            "Epoch [4/10], Step [31/97], Loss: 0.2694\n",
            "Epoch [4/10], Step [32/97], Loss: 0.3459\n",
            "Epoch [4/10], Step [33/97], Loss: 0.3157\n",
            "Epoch [4/10], Step [34/97], Loss: 0.2909\n",
            "Epoch [4/10], Step [35/97], Loss: 0.4786\n",
            "Epoch [4/10], Step [36/97], Loss: 0.4248\n",
            "Epoch [4/10], Step [37/97], Loss: 0.1574\n",
            "Epoch [4/10], Step [38/97], Loss: 0.2823\n",
            "Epoch [4/10], Step [39/97], Loss: 0.2872\n",
            "Epoch [4/10], Step [40/97], Loss: 0.2007\n",
            "Epoch [4/10], Step [41/97], Loss: 0.3884\n",
            "Epoch [4/10], Step [42/97], Loss: 0.3352\n",
            "Epoch [4/10], Step [43/97], Loss: 0.2724\n",
            "Epoch [4/10], Step [44/97], Loss: 0.1578\n",
            "Epoch [4/10], Step [45/97], Loss: 0.3666\n",
            "Epoch [4/10], Step [46/97], Loss: 0.2420\n",
            "Epoch [4/10], Step [47/97], Loss: 0.1781\n",
            "Epoch [4/10], Step [48/97], Loss: 0.2941\n",
            "Epoch [4/10], Step [49/97], Loss: 0.3716\n",
            "Epoch [4/10], Step [50/97], Loss: 0.2328\n",
            "Epoch [4/10], Step [51/97], Loss: 0.2260\n",
            "Epoch [4/10], Step [52/97], Loss: 0.2286\n",
            "Epoch [4/10], Step [53/97], Loss: 0.3900\n",
            "Epoch [4/10], Step [54/97], Loss: 0.3716\n",
            "Epoch [4/10], Step [55/97], Loss: 0.3561\n",
            "Epoch [4/10], Step [56/97], Loss: 0.2362\n",
            "Epoch [4/10], Step [57/97], Loss: 0.5842\n",
            "Epoch [4/10], Step [58/97], Loss: 0.3984\n",
            "Epoch [4/10], Step [59/97], Loss: 0.2282\n",
            "Epoch [4/10], Step [60/97], Loss: 0.3339\n",
            "Epoch [4/10], Step [61/97], Loss: 0.4180\n",
            "Epoch [4/10], Step [62/97], Loss: 0.1837\n",
            "Epoch [4/10], Step [63/97], Loss: 0.2303\n",
            "Epoch [4/10], Step [64/97], Loss: 0.4440\n",
            "Epoch [4/10], Step [65/97], Loss: 0.4403\n",
            "Epoch [4/10], Step [66/97], Loss: 0.3264\n",
            "Epoch [4/10], Step [67/97], Loss: 0.5088\n",
            "Epoch [4/10], Step [68/97], Loss: 0.1923\n",
            "Epoch [4/10], Step [69/97], Loss: 0.3255\n",
            "Epoch [4/10], Step [70/97], Loss: 0.1858\n",
            "Epoch [4/10], Step [71/97], Loss: 0.2598\n",
            "Epoch [4/10], Step [72/97], Loss: 0.3282\n",
            "Epoch [4/10], Step [73/97], Loss: 0.4545\n",
            "Epoch [4/10], Step [74/97], Loss: 0.2838\n",
            "Epoch [4/10], Step [75/97], Loss: 0.3238\n",
            "Epoch [4/10], Step [76/97], Loss: 0.2197\n",
            "Epoch [4/10], Step [77/97], Loss: 0.4347\n",
            "Epoch [4/10], Step [78/97], Loss: 0.3499\n",
            "Epoch [4/10], Step [79/97], Loss: 0.3383\n",
            "Epoch [4/10], Step [80/97], Loss: 0.2138\n",
            "Epoch [4/10], Step [81/97], Loss: 0.3777\n",
            "Epoch [4/10], Step [82/97], Loss: 0.4982\n",
            "Epoch [4/10], Step [83/97], Loss: 0.5154\n",
            "Epoch [4/10], Step [84/97], Loss: 0.2469\n",
            "Epoch [4/10], Step [85/97], Loss: 0.3484\n",
            "Epoch [4/10], Step [86/97], Loss: 0.6389\n",
            "Epoch [4/10], Step [87/97], Loss: 0.2118\n",
            "Epoch [4/10], Step [88/97], Loss: 0.2721\n",
            "Epoch [4/10], Step [89/97], Loss: 0.2611\n",
            "Epoch [4/10], Step [90/97], Loss: 0.3656\n",
            "Epoch [4/10], Step [91/97], Loss: 0.2714\n",
            "Epoch [4/10], Step [92/97], Loss: 0.3409\n",
            "Epoch [4/10], Step [93/97], Loss: 0.5068\n",
            "Epoch [4/10], Step [94/97], Loss: 0.4352\n",
            "Epoch [4/10], Step [95/97], Loss: 0.2420\n",
            "Epoch [4/10], Step [96/97], Loss: 0.4060\n",
            "Epoch [4/10], Step [97/97], Loss: 0.2241\n",
            "Epoch finished\n",
            "Epoch [5/10], Step [1/97], Loss: 0.3794\n",
            "Epoch [5/10], Step [2/97], Loss: 0.2218\n",
            "Epoch [5/10], Step [3/97], Loss: 0.3932\n",
            "Epoch [5/10], Step [4/97], Loss: 0.3364\n",
            "Epoch [5/10], Step [5/97], Loss: 0.3360\n",
            "Epoch [5/10], Step [6/97], Loss: 0.3399\n",
            "Epoch [5/10], Step [7/97], Loss: 0.3324\n",
            "Epoch [5/10], Step [8/97], Loss: 0.1429\n",
            "Epoch [5/10], Step [9/97], Loss: 0.2629\n",
            "Epoch [5/10], Step [10/97], Loss: 0.1407\n",
            "Epoch [5/10], Step [11/97], Loss: 0.4600\n",
            "Epoch [5/10], Step [12/97], Loss: 0.2315\n",
            "Epoch [5/10], Step [13/97], Loss: 0.3585\n",
            "Epoch [5/10], Step [14/97], Loss: 0.3443\n",
            "Epoch [5/10], Step [15/97], Loss: 0.4879\n",
            "Epoch [5/10], Step [16/97], Loss: 0.2047\n",
            "Epoch [5/10], Step [17/97], Loss: 0.1988\n",
            "Epoch [5/10], Step [18/97], Loss: 0.3029\n",
            "Epoch [5/10], Step [19/97], Loss: 0.2986\n",
            "Epoch [5/10], Step [20/97], Loss: 0.3264\n",
            "Epoch [5/10], Step [21/97], Loss: 0.4786\n",
            "Epoch [5/10], Step [22/97], Loss: 0.4323\n",
            "Epoch [5/10], Step [23/97], Loss: 0.2620\n",
            "Epoch [5/10], Step [24/97], Loss: 0.3093\n",
            "Epoch [5/10], Step [25/97], Loss: 0.2346\n",
            "Epoch [5/10], Step [26/97], Loss: 0.5181\n",
            "Epoch [5/10], Step [27/97], Loss: 0.1844\n",
            "Epoch [5/10], Step [28/97], Loss: 0.3457\n",
            "Epoch [5/10], Step [29/97], Loss: 0.3406\n",
            "Epoch [5/10], Step [30/97], Loss: 0.4426\n",
            "Epoch [5/10], Step [31/97], Loss: 0.2965\n",
            "Epoch [5/10], Step [32/97], Loss: 0.2605\n",
            "Epoch [5/10], Step [33/97], Loss: 0.2854\n",
            "Epoch [5/10], Step [34/97], Loss: 0.2672\n",
            "Epoch [5/10], Step [35/97], Loss: 0.4077\n",
            "Epoch [5/10], Step [36/97], Loss: 0.3897\n",
            "Epoch [5/10], Step [37/97], Loss: 0.1363\n",
            "Epoch [5/10], Step [38/97], Loss: 0.1854\n",
            "Epoch [5/10], Step [39/97], Loss: 0.3211\n",
            "Epoch [5/10], Step [40/97], Loss: 0.2628\n",
            "Epoch [5/10], Step [41/97], Loss: 0.4389\n",
            "Epoch [5/10], Step [42/97], Loss: 0.2018\n",
            "Epoch [5/10], Step [43/97], Loss: 0.2325\n",
            "Epoch [5/10], Step [44/97], Loss: 0.1636\n",
            "Epoch [5/10], Step [45/97], Loss: 0.2812\n",
            "Epoch [5/10], Step [46/97], Loss: 0.3628\n",
            "Epoch [5/10], Step [47/97], Loss: 0.1789\n",
            "Epoch [5/10], Step [48/97], Loss: 0.3186\n",
            "Epoch [5/10], Step [49/97], Loss: 0.2856\n",
            "Epoch [5/10], Step [50/97], Loss: 0.2666\n",
            "Epoch [5/10], Step [51/97], Loss: 0.3716\n",
            "Epoch [5/10], Step [52/97], Loss: 0.2701\n",
            "Epoch [5/10], Step [53/97], Loss: 0.3719\n",
            "Epoch [5/10], Step [54/97], Loss: 0.1921\n",
            "Epoch [5/10], Step [55/97], Loss: 0.1961\n",
            "Epoch [5/10], Step [56/97], Loss: 0.2361\n",
            "Epoch [5/10], Step [57/97], Loss: 0.3546\n",
            "Epoch [5/10], Step [58/97], Loss: 0.3470\n",
            "Epoch [5/10], Step [59/97], Loss: 0.2349\n",
            "Epoch [5/10], Step [60/97], Loss: 0.3452\n",
            "Epoch [5/10], Step [61/97], Loss: 0.2278\n",
            "Epoch [5/10], Step [62/97], Loss: 0.1553\n",
            "Epoch [5/10], Step [63/97], Loss: 0.1957\n",
            "Epoch [5/10], Step [64/97], Loss: 0.2545\n",
            "Epoch [5/10], Step [65/97], Loss: 0.5033\n",
            "Epoch [5/10], Step [66/97], Loss: 0.2625\n",
            "Epoch [5/10], Step [67/97], Loss: 0.3971\n",
            "Epoch [5/10], Step [68/97], Loss: 0.2264\n",
            "Epoch [5/10], Step [69/97], Loss: 0.4581\n",
            "Epoch [5/10], Step [70/97], Loss: 0.2354\n",
            "Epoch [5/10], Step [71/97], Loss: 0.3125\n",
            "Epoch [5/10], Step [72/97], Loss: 0.3000\n",
            "Epoch [5/10], Step [73/97], Loss: 0.4571\n",
            "Epoch [5/10], Step [74/97], Loss: 0.3746\n",
            "Epoch [5/10], Step [75/97], Loss: 0.3926\n",
            "Epoch [5/10], Step [76/97], Loss: 0.2911\n",
            "Epoch [5/10], Step [77/97], Loss: 0.4465\n",
            "Epoch [5/10], Step [78/97], Loss: 0.3001\n",
            "Epoch [5/10], Step [79/97], Loss: 0.2971\n",
            "Epoch [5/10], Step [80/97], Loss: 0.2493\n",
            "Epoch [5/10], Step [81/97], Loss: 0.4181\n",
            "Epoch [5/10], Step [82/97], Loss: 0.2539\n",
            "Epoch [5/10], Step [83/97], Loss: 0.3176\n",
            "Epoch [5/10], Step [84/97], Loss: 0.2142\n",
            "Epoch [5/10], Step [85/97], Loss: 0.3437\n",
            "Epoch [5/10], Step [86/97], Loss: 0.3982\n",
            "Epoch [5/10], Step [87/97], Loss: 0.2316\n",
            "Epoch [5/10], Step [88/97], Loss: 0.1877\n",
            "Epoch [5/10], Step [89/97], Loss: 0.2523\n",
            "Epoch [5/10], Step [90/97], Loss: 0.3798\n",
            "Epoch [5/10], Step [91/97], Loss: 0.2023\n",
            "Epoch [5/10], Step [92/97], Loss: 0.2681\n",
            "Epoch [5/10], Step [93/97], Loss: 0.4453\n",
            "Epoch [5/10], Step [94/97], Loss: 0.2978\n",
            "Epoch [5/10], Step [95/97], Loss: 0.1551\n",
            "Epoch [5/10], Step [96/97], Loss: 0.4190\n",
            "Epoch [5/10], Step [97/97], Loss: 0.1449\n",
            "Epoch finished\n",
            "Epoch [6/10], Step [1/97], Loss: 0.2374\n",
            "Epoch [6/10], Step [2/97], Loss: 0.2543\n",
            "Epoch [6/10], Step [3/97], Loss: 0.4167\n",
            "Epoch [6/10], Step [4/97], Loss: 0.2778\n",
            "Epoch [6/10], Step [5/97], Loss: 0.3285\n",
            "Epoch [6/10], Step [6/97], Loss: 0.1695\n",
            "Epoch [6/10], Step [7/97], Loss: 0.4034\n",
            "Epoch [6/10], Step [8/97], Loss: 0.1994\n",
            "Epoch [6/10], Step [9/97], Loss: 0.2701\n",
            "Epoch [6/10], Step [10/97], Loss: 0.2251\n",
            "Epoch [6/10], Step [11/97], Loss: 0.4137\n",
            "Epoch [6/10], Step [12/97], Loss: 0.3331\n",
            "Epoch [6/10], Step [13/97], Loss: 0.2572\n",
            "Epoch [6/10], Step [14/97], Loss: 0.2172\n",
            "Epoch [6/10], Step [15/97], Loss: 0.3860\n",
            "Epoch [6/10], Step [16/97], Loss: 0.1689\n",
            "Epoch [6/10], Step [17/97], Loss: 0.4514\n",
            "Epoch [6/10], Step [18/97], Loss: 0.2773\n",
            "Epoch [6/10], Step [19/97], Loss: 0.2445\n",
            "Epoch [6/10], Step [20/97], Loss: 0.2854\n",
            "Epoch [6/10], Step [21/97], Loss: 0.2907\n",
            "Epoch [6/10], Step [22/97], Loss: 0.3143\n",
            "Epoch [6/10], Step [23/97], Loss: 0.1249\n",
            "Epoch [6/10], Step [24/97], Loss: 0.2414\n",
            "Epoch [6/10], Step [25/97], Loss: 0.2586\n",
            "Epoch [6/10], Step [26/97], Loss: 0.3372\n",
            "Epoch [6/10], Step [27/97], Loss: 0.1150\n",
            "Epoch [6/10], Step [28/97], Loss: 0.2091\n",
            "Epoch [6/10], Step [29/97], Loss: 0.1664\n",
            "Epoch [6/10], Step [30/97], Loss: 0.3218\n",
            "Epoch [6/10], Step [31/97], Loss: 0.2478\n",
            "Epoch [6/10], Step [32/97], Loss: 0.2752\n",
            "Epoch [6/10], Step [33/97], Loss: 0.2048\n",
            "Epoch [6/10], Step [34/97], Loss: 0.1846\n",
            "Epoch [6/10], Step [35/97], Loss: 0.2354\n",
            "Epoch [6/10], Step [36/97], Loss: 0.3068\n",
            "Epoch [6/10], Step [37/97], Loss: 0.1113\n",
            "Epoch [6/10], Step [38/97], Loss: 0.2199\n",
            "Epoch [6/10], Step [39/97], Loss: 0.2424\n",
            "Epoch [6/10], Step [40/97], Loss: 0.2233\n",
            "Epoch [6/10], Step [41/97], Loss: 0.4245\n",
            "Epoch [6/10], Step [42/97], Loss: 0.3099\n",
            "Epoch [6/10], Step [43/97], Loss: 0.2625\n",
            "Epoch [6/10], Step [44/97], Loss: 0.1429\n",
            "Epoch [6/10], Step [45/97], Loss: 0.2182\n",
            "Epoch [6/10], Step [46/97], Loss: 0.1666\n",
            "Epoch [6/10], Step [47/97], Loss: 0.1347\n",
            "Epoch [6/10], Step [48/97], Loss: 0.2859\n",
            "Epoch [6/10], Step [49/97], Loss: 0.2982\n",
            "Epoch [6/10], Step [50/97], Loss: 0.2146\n",
            "Epoch [6/10], Step [51/97], Loss: 0.2675\n",
            "Epoch [6/10], Step [52/97], Loss: 0.1644\n",
            "Epoch [6/10], Step [53/97], Loss: 0.3264\n",
            "Epoch [6/10], Step [54/97], Loss: 0.2144\n",
            "Epoch [6/10], Step [55/97], Loss: 0.2412\n",
            "Epoch [6/10], Step [56/97], Loss: 0.2607\n",
            "Epoch [6/10], Step [57/97], Loss: 0.4531\n",
            "Epoch [6/10], Step [58/97], Loss: 0.4030\n",
            "Epoch [6/10], Step [59/97], Loss: 0.2210\n",
            "Epoch [6/10], Step [60/97], Loss: 0.4696\n",
            "Epoch [6/10], Step [61/97], Loss: 0.2519\n",
            "Epoch [6/10], Step [62/97], Loss: 0.2163\n",
            "Epoch [6/10], Step [63/97], Loss: 0.2120\n",
            "Epoch [6/10], Step [64/97], Loss: 0.2999\n",
            "Epoch [6/10], Step [65/97], Loss: 0.4477\n",
            "Epoch [6/10], Step [66/97], Loss: 0.1545\n",
            "Epoch [6/10], Step [67/97], Loss: 0.4862\n",
            "Epoch [6/10], Step [68/97], Loss: 0.2364\n",
            "Epoch [6/10], Step [69/97], Loss: 0.3790\n",
            "Epoch [6/10], Step [70/97], Loss: 0.2297\n",
            "Epoch [6/10], Step [71/97], Loss: 0.2315\n",
            "Epoch [6/10], Step [72/97], Loss: 0.3153\n",
            "Epoch [6/10], Step [73/97], Loss: 0.4050\n",
            "Epoch [6/10], Step [74/97], Loss: 0.2986\n",
            "Epoch [6/10], Step [75/97], Loss: 0.3396\n",
            "Epoch [6/10], Step [76/97], Loss: 0.1654\n",
            "Epoch [6/10], Step [77/97], Loss: 0.3395\n",
            "Epoch [6/10], Step [78/97], Loss: 0.3447\n",
            "Epoch [6/10], Step [79/97], Loss: 0.2536\n",
            "Epoch [6/10], Step [80/97], Loss: 0.1757\n",
            "Epoch [6/10], Step [81/97], Loss: 0.3275\n",
            "Epoch [6/10], Step [82/97], Loss: 0.3296\n",
            "Epoch [6/10], Step [83/97], Loss: 0.2846\n",
            "Epoch [6/10], Step [84/97], Loss: 0.3574\n",
            "Epoch [6/10], Step [85/97], Loss: 0.3658\n",
            "Epoch [6/10], Step [86/97], Loss: 0.4083\n",
            "Epoch [6/10], Step [87/97], Loss: 0.1303\n",
            "Epoch [6/10], Step [88/97], Loss: 0.1858\n",
            "Epoch [6/10], Step [89/97], Loss: 0.3498\n",
            "Epoch [6/10], Step [90/97], Loss: 0.4462\n",
            "Epoch [6/10], Step [91/97], Loss: 0.1999\n",
            "Epoch [6/10], Step [92/97], Loss: 0.2451\n",
            "Epoch [6/10], Step [93/97], Loss: 0.3526\n",
            "Epoch [6/10], Step [94/97], Loss: 0.2591\n",
            "Epoch [6/10], Step [95/97], Loss: 0.1298\n",
            "Epoch [6/10], Step [96/97], Loss: 0.2999\n",
            "Epoch [6/10], Step [97/97], Loss: 0.1211\n",
            "Epoch finished\n",
            "Epoch [7/10], Step [1/97], Loss: 0.2409\n",
            "Epoch [7/10], Step [2/97], Loss: 0.3587\n",
            "Epoch [7/10], Step [3/97], Loss: 0.2568\n",
            "Epoch [7/10], Step [4/97], Loss: 0.3092\n",
            "Epoch [7/10], Step [5/97], Loss: 0.2800\n",
            "Epoch [7/10], Step [6/97], Loss: 0.3198\n",
            "Epoch [7/10], Step [7/97], Loss: 0.4399\n",
            "Epoch [7/10], Step [8/97], Loss: 0.1733\n",
            "Epoch [7/10], Step [9/97], Loss: 0.2368\n",
            "Epoch [7/10], Step [10/97], Loss: 0.3517\n",
            "Epoch [7/10], Step [11/97], Loss: 0.3812\n",
            "Epoch [7/10], Step [12/97], Loss: 0.2711\n",
            "Epoch [7/10], Step [13/97], Loss: 0.2838\n",
            "Epoch [7/10], Step [14/97], Loss: 0.2279\n",
            "Epoch [7/10], Step [15/97], Loss: 0.6888\n",
            "Epoch [7/10], Step [16/97], Loss: 0.1303\n",
            "Epoch [7/10], Step [17/97], Loss: 0.2100\n",
            "Epoch [7/10], Step [18/97], Loss: 0.3130\n",
            "Epoch [7/10], Step [19/97], Loss: 0.2700\n",
            "Epoch [7/10], Step [20/97], Loss: 0.2184\n",
            "Epoch [7/10], Step [21/97], Loss: 0.4270\n",
            "Epoch [7/10], Step [22/97], Loss: 0.2299\n",
            "Epoch [7/10], Step [23/97], Loss: 0.1686\n",
            "Epoch [7/10], Step [24/97], Loss: 0.1990\n",
            "Epoch [7/10], Step [25/97], Loss: 0.2146\n",
            "Epoch [7/10], Step [26/97], Loss: 0.4927\n",
            "Epoch [7/10], Step [27/97], Loss: 0.2023\n",
            "Epoch [7/10], Step [28/97], Loss: 0.3865\n",
            "Epoch [7/10], Step [29/97], Loss: 0.2304\n",
            "Epoch [7/10], Step [30/97], Loss: 0.4362\n",
            "Epoch [7/10], Step [31/97], Loss: 0.1520\n",
            "Epoch [7/10], Step [32/97], Loss: 0.2393\n",
            "Epoch [7/10], Step [33/97], Loss: 0.3522\n",
            "Epoch [7/10], Step [34/97], Loss: 0.3207\n",
            "Epoch [7/10], Step [35/97], Loss: 0.2811\n",
            "Epoch [7/10], Step [36/97], Loss: 0.2483\n",
            "Epoch [7/10], Step [37/97], Loss: 0.1815\n",
            "Epoch [7/10], Step [38/97], Loss: 0.1376\n",
            "Epoch [7/10], Step [39/97], Loss: 0.1826\n",
            "Epoch [7/10], Step [40/97], Loss: 0.1198\n",
            "Epoch [7/10], Step [41/97], Loss: 0.2651\n",
            "Epoch [7/10], Step [42/97], Loss: 0.3166\n",
            "Epoch [7/10], Step [43/97], Loss: 0.1928\n",
            "Epoch [7/10], Step [44/97], Loss: 0.1596\n",
            "Epoch [7/10], Step [45/97], Loss: 0.2168\n",
            "Epoch [7/10], Step [46/97], Loss: 0.2001\n",
            "Epoch [7/10], Step [47/97], Loss: 0.1456\n",
            "Epoch [7/10], Step [48/97], Loss: 0.1955\n",
            "Epoch [7/10], Step [49/97], Loss: 0.1797\n",
            "Epoch [7/10], Step [50/97], Loss: 0.2358\n",
            "Epoch [7/10], Step [51/97], Loss: 0.3026\n",
            "Epoch [7/10], Step [52/97], Loss: 0.1935\n",
            "Epoch [7/10], Step [53/97], Loss: 0.2766\n",
            "Epoch [7/10], Step [54/97], Loss: 0.0858\n",
            "Epoch [7/10], Step [55/97], Loss: 0.2445\n",
            "Epoch [7/10], Step [56/97], Loss: 0.1575\n",
            "Epoch [7/10], Step [57/97], Loss: 0.1497\n",
            "Epoch [7/10], Step [58/97], Loss: 0.1440\n",
            "Epoch [7/10], Step [59/97], Loss: 0.1174\n",
            "Epoch [7/10], Step [60/97], Loss: 0.2185\n",
            "Epoch [7/10], Step [61/97], Loss: 0.2118\n",
            "Epoch [7/10], Step [62/97], Loss: 0.1704\n",
            "Epoch [7/10], Step [63/97], Loss: 0.0720\n",
            "Epoch [7/10], Step [64/97], Loss: 0.2426\n",
            "Epoch [7/10], Step [65/97], Loss: 0.4519\n",
            "Epoch [7/10], Step [66/97], Loss: 0.1750\n",
            "Epoch [7/10], Step [67/97], Loss: 0.3405\n",
            "Epoch [7/10], Step [68/97], Loss: 0.1882\n",
            "Epoch [7/10], Step [69/97], Loss: 0.3362\n",
            "Epoch [7/10], Step [70/97], Loss: 0.1087\n",
            "Epoch [7/10], Step [71/97], Loss: 0.2281\n",
            "Epoch [7/10], Step [72/97], Loss: 0.2661\n",
            "Epoch [7/10], Step [73/97], Loss: 0.3605\n",
            "Epoch [7/10], Step [74/97], Loss: 0.3041\n",
            "Epoch [7/10], Step [75/97], Loss: 0.2347\n",
            "Epoch [7/10], Step [76/97], Loss: 0.1289\n",
            "Epoch [7/10], Step [77/97], Loss: 0.4295\n",
            "Epoch [7/10], Step [78/97], Loss: 0.2447\n",
            "Epoch [7/10], Step [79/97], Loss: 0.1596\n",
            "Epoch [7/10], Step [80/97], Loss: 0.1906\n",
            "Epoch [7/10], Step [81/97], Loss: 0.2854\n",
            "Epoch [7/10], Step [82/97], Loss: 0.3187\n",
            "Epoch [7/10], Step [83/97], Loss: 0.6038\n",
            "Epoch [7/10], Step [84/97], Loss: 0.1458\n",
            "Epoch [7/10], Step [85/97], Loss: 0.2792\n",
            "Epoch [7/10], Step [86/97], Loss: 0.4670\n",
            "Epoch [7/10], Step [87/97], Loss: 0.1093\n",
            "Epoch [7/10], Step [88/97], Loss: 0.1539\n",
            "Epoch [7/10], Step [89/97], Loss: 0.2561\n",
            "Epoch [7/10], Step [90/97], Loss: 0.2988\n",
            "Epoch [7/10], Step [91/97], Loss: 0.2315\n",
            "Epoch [7/10], Step [92/97], Loss: 0.2734\n",
            "Epoch [7/10], Step [93/97], Loss: 0.3291\n",
            "Epoch [7/10], Step [94/97], Loss: 0.2349\n",
            "Epoch [7/10], Step [95/97], Loss: 0.2138\n",
            "Epoch [7/10], Step [96/97], Loss: 0.3549\n",
            "Epoch [7/10], Step [97/97], Loss: 0.2006\n",
            "Epoch finished\n",
            "Epoch [8/10], Step [1/97], Loss: 0.1184\n",
            "Epoch [8/10], Step [2/97], Loss: 0.2785\n",
            "Epoch [8/10], Step [3/97], Loss: 0.3552\n",
            "Epoch [8/10], Step [4/97], Loss: 0.1803\n",
            "Epoch [8/10], Step [5/97], Loss: 0.1639\n",
            "Epoch [8/10], Step [6/97], Loss: 0.2680\n",
            "Epoch [8/10], Step [7/97], Loss: 0.2680\n",
            "Epoch [8/10], Step [8/97], Loss: 0.1690\n",
            "Epoch [8/10], Step [9/97], Loss: 0.1861\n",
            "Epoch [8/10], Step [10/97], Loss: 0.0984\n",
            "Epoch [8/10], Step [11/97], Loss: 0.4195\n",
            "Epoch [8/10], Step [12/97], Loss: 0.2926\n",
            "Epoch [8/10], Step [13/97], Loss: 0.2581\n",
            "Epoch [8/10], Step [14/97], Loss: 0.1424\n",
            "Epoch [8/10], Step [15/97], Loss: 0.3698\n",
            "Epoch [8/10], Step [16/97], Loss: 0.0937\n",
            "Epoch [8/10], Step [17/97], Loss: 0.1941\n",
            "Epoch [8/10], Step [18/97], Loss: 0.3219\n",
            "Epoch [8/10], Step [19/97], Loss: 0.1617\n",
            "Epoch [8/10], Step [20/97], Loss: 0.2329\n",
            "Epoch [8/10], Step [21/97], Loss: 0.2874\n",
            "Epoch [8/10], Step [22/97], Loss: 0.1674\n",
            "Epoch [8/10], Step [23/97], Loss: 0.1233\n",
            "Epoch [8/10], Step [24/97], Loss: 0.2262\n",
            "Epoch [8/10], Step [25/97], Loss: 0.1605\n",
            "Epoch [8/10], Step [26/97], Loss: 0.2445\n",
            "Epoch [8/10], Step [27/97], Loss: 0.1476\n",
            "Epoch [8/10], Step [28/97], Loss: 0.2052\n",
            "Epoch [8/10], Step [29/97], Loss: 0.1770\n",
            "Epoch [8/10], Step [30/97], Loss: 0.2848\n",
            "Epoch [8/10], Step [31/97], Loss: 0.1245\n",
            "Epoch [8/10], Step [32/97], Loss: 0.1704\n",
            "Epoch [8/10], Step [33/97], Loss: 0.1514\n",
            "Epoch [8/10], Step [34/97], Loss: 0.1483\n",
            "Epoch [8/10], Step [35/97], Loss: 0.1949\n",
            "Epoch [8/10], Step [36/97], Loss: 0.1960\n",
            "Epoch [8/10], Step [37/97], Loss: 0.1241\n",
            "Epoch [8/10], Step [38/97], Loss: 0.1396\n",
            "Epoch [8/10], Step [39/97], Loss: 0.1490\n",
            "Epoch [8/10], Step [40/97], Loss: 0.1087\n",
            "Epoch [8/10], Step [41/97], Loss: 0.2674\n",
            "Epoch [8/10], Step [42/97], Loss: 0.1707\n",
            "Epoch [8/10], Step [43/97], Loss: 0.1197\n",
            "Epoch [8/10], Step [44/97], Loss: 0.1262\n",
            "Epoch [8/10], Step [45/97], Loss: 0.1429\n",
            "Epoch [8/10], Step [46/97], Loss: 0.1203\n",
            "Epoch [8/10], Step [47/97], Loss: 0.1211\n",
            "Epoch [8/10], Step [48/97], Loss: 0.2620\n",
            "Epoch [8/10], Step [49/97], Loss: 0.1675\n",
            "Epoch [8/10], Step [50/97], Loss: 0.2559\n",
            "Epoch [8/10], Step [51/97], Loss: 0.3729\n",
            "Epoch [8/10], Step [52/97], Loss: 0.2916\n",
            "Epoch [8/10], Step [53/97], Loss: 0.2994\n",
            "Epoch [8/10], Step [54/97], Loss: 0.0916\n",
            "Epoch [8/10], Step [55/97], Loss: 0.2074\n",
            "Epoch [8/10], Step [56/97], Loss: 0.1376\n",
            "Epoch [8/10], Step [57/97], Loss: 0.2146\n",
            "Epoch [8/10], Step [58/97], Loss: 0.2870\n",
            "Epoch [8/10], Step [59/97], Loss: 0.1156\n",
            "Epoch [8/10], Step [60/97], Loss: 0.1413\n",
            "Epoch [8/10], Step [61/97], Loss: 0.2195\n",
            "Epoch [8/10], Step [62/97], Loss: 0.2503\n",
            "Epoch [8/10], Step [63/97], Loss: 0.1358\n",
            "Epoch [8/10], Step [64/97], Loss: 0.2660\n",
            "Epoch [8/10], Step [65/97], Loss: 0.4035\n",
            "Epoch [8/10], Step [66/97], Loss: 0.1735\n",
            "Epoch [8/10], Step [67/97], Loss: 0.4808\n",
            "Epoch [8/10], Step [68/97], Loss: 0.3007\n",
            "Epoch [8/10], Step [69/97], Loss: 0.3397\n",
            "Epoch [8/10], Step [70/97], Loss: 0.1464\n",
            "Epoch [8/10], Step [71/97], Loss: 0.1828\n",
            "Epoch [8/10], Step [72/97], Loss: 0.1705\n",
            "Epoch [8/10], Step [73/97], Loss: 0.3362\n",
            "Epoch [8/10], Step [74/97], Loss: 0.2221\n",
            "Epoch [8/10], Step [75/97], Loss: 0.2930\n",
            "Epoch [8/10], Step [76/97], Loss: 0.1320\n",
            "Epoch [8/10], Step [77/97], Loss: 0.2307\n",
            "Epoch [8/10], Step [78/97], Loss: 0.4384\n",
            "Epoch [8/10], Step [79/97], Loss: 0.2139\n",
            "Epoch [8/10], Step [80/97], Loss: 0.1646\n",
            "Epoch [8/10], Step [81/97], Loss: 0.2590\n",
            "Epoch [8/10], Step [82/97], Loss: 0.2421\n",
            "Epoch [8/10], Step [83/97], Loss: 0.2264\n",
            "Epoch [8/10], Step [84/97], Loss: 0.1629\n",
            "Epoch [8/10], Step [85/97], Loss: 0.1945\n",
            "Epoch [8/10], Step [86/97], Loss: 0.3697\n",
            "Epoch [8/10], Step [87/97], Loss: 0.2128\n",
            "Epoch [8/10], Step [88/97], Loss: 0.1720\n",
            "Epoch [8/10], Step [89/97], Loss: 0.2636\n",
            "Epoch [8/10], Step [90/97], Loss: 0.2574\n",
            "Epoch [8/10], Step [91/97], Loss: 0.2179\n",
            "Epoch [8/10], Step [92/97], Loss: 0.2098\n",
            "Epoch [8/10], Step [93/97], Loss: 0.3891\n",
            "Epoch [8/10], Step [94/97], Loss: 0.2152\n",
            "Epoch [8/10], Step [95/97], Loss: 0.1313\n",
            "Epoch [8/10], Step [96/97], Loss: 0.3396\n",
            "Epoch [8/10], Step [97/97], Loss: 0.1422\n",
            "Epoch finished\n",
            "Epoch [9/10], Step [1/97], Loss: 0.1147\n",
            "Epoch [9/10], Step [2/97], Loss: 0.3082\n",
            "Epoch [9/10], Step [3/97], Loss: 0.2158\n",
            "Epoch [9/10], Step [4/97], Loss: 0.2328\n",
            "Epoch [9/10], Step [5/97], Loss: 0.2408\n",
            "Epoch [9/10], Step [6/97], Loss: 0.1850\n",
            "Epoch [9/10], Step [7/97], Loss: 0.2994\n",
            "Epoch [9/10], Step [8/97], Loss: 0.1480\n",
            "Epoch [9/10], Step [9/97], Loss: 0.2226\n",
            "Epoch [9/10], Step [10/97], Loss: 0.0782\n",
            "Epoch [9/10], Step [11/97], Loss: 0.3140\n",
            "Epoch [9/10], Step [12/97], Loss: 0.1334\n",
            "Epoch [9/10], Step [13/97], Loss: 0.4057\n",
            "Epoch [9/10], Step [14/97], Loss: 0.1751\n",
            "Epoch [9/10], Step [15/97], Loss: 0.3022\n",
            "Epoch [9/10], Step [16/97], Loss: 0.1054\n",
            "Epoch [9/10], Step [17/97], Loss: 0.1468\n",
            "Epoch [9/10], Step [18/97], Loss: 0.1435\n",
            "Epoch [9/10], Step [19/97], Loss: 0.1879\n",
            "Epoch [9/10], Step [20/97], Loss: 0.1254\n",
            "Epoch [9/10], Step [21/97], Loss: 0.3530\n",
            "Epoch [9/10], Step [22/97], Loss: 0.1735\n",
            "Epoch [9/10], Step [23/97], Loss: 0.0650\n",
            "Epoch [9/10], Step [24/97], Loss: 0.2416\n",
            "Epoch [9/10], Step [25/97], Loss: 0.0751\n",
            "Epoch [9/10], Step [26/97], Loss: 0.2013\n",
            "Epoch [9/10], Step [27/97], Loss: 0.1237\n",
            "Epoch [9/10], Step [28/97], Loss: 0.3735\n",
            "Epoch [9/10], Step [29/97], Loss: 0.2561\n",
            "Epoch [9/10], Step [30/97], Loss: 0.3728\n",
            "Epoch [9/10], Step [31/97], Loss: 0.1507\n",
            "Epoch [9/10], Step [32/97], Loss: 0.1814\n",
            "Epoch [9/10], Step [33/97], Loss: 0.1315\n",
            "Epoch [9/10], Step [34/97], Loss: 0.1875\n",
            "Epoch [9/10], Step [35/97], Loss: 0.2297\n",
            "Epoch [9/10], Step [36/97], Loss: 0.3179\n",
            "Epoch [9/10], Step [37/97], Loss: 0.0919\n",
            "Epoch [9/10], Step [38/97], Loss: 0.2041\n",
            "Epoch [9/10], Step [39/97], Loss: 0.1027\n",
            "Epoch [9/10], Step [40/97], Loss: 0.3260\n",
            "Epoch [9/10], Step [41/97], Loss: 0.2915\n",
            "Epoch [9/10], Step [42/97], Loss: 0.2127\n",
            "Epoch [9/10], Step [43/97], Loss: 0.1907\n",
            "Epoch [9/10], Step [44/97], Loss: 0.1658\n",
            "Epoch [9/10], Step [45/97], Loss: 0.2319\n",
            "Epoch [9/10], Step [46/97], Loss: 0.1515\n",
            "Epoch [9/10], Step [47/97], Loss: 0.0911\n",
            "Epoch [9/10], Step [48/97], Loss: 0.3783\n",
            "Epoch [9/10], Step [49/97], Loss: 0.2122\n",
            "Epoch [9/10], Step [50/97], Loss: 0.1649\n",
            "Epoch [9/10], Step [51/97], Loss: 0.2626\n",
            "Epoch [9/10], Step [52/97], Loss: 0.1256\n",
            "Epoch [9/10], Step [53/97], Loss: 0.1628\n",
            "Epoch [9/10], Step [54/97], Loss: 0.1513\n",
            "Epoch [9/10], Step [55/97], Loss: 0.1328\n",
            "Epoch [9/10], Step [56/97], Loss: 0.1300\n",
            "Epoch [9/10], Step [57/97], Loss: 0.1675\n",
            "Epoch [9/10], Step [58/97], Loss: 0.1487\n",
            "Epoch [9/10], Step [59/97], Loss: 0.1413\n",
            "Epoch [9/10], Step [60/97], Loss: 0.1324\n",
            "Epoch [9/10], Step [61/97], Loss: 0.1111\n",
            "Epoch [9/10], Step [62/97], Loss: 0.0980\n",
            "Epoch [9/10], Step [63/97], Loss: 0.1201\n",
            "Epoch [9/10], Step [64/97], Loss: 0.1646\n",
            "Epoch [9/10], Step [65/97], Loss: 0.3180\n",
            "Epoch [9/10], Step [66/97], Loss: 0.1120\n",
            "Epoch [9/10], Step [67/97], Loss: 0.3282\n",
            "Epoch [9/10], Step [68/97], Loss: 0.1866\n",
            "Epoch [9/10], Step [69/97], Loss: 0.3438\n",
            "Epoch [9/10], Step [70/97], Loss: 0.1714\n",
            "Epoch [9/10], Step [71/97], Loss: 0.1323\n",
            "Epoch [9/10], Step [72/97], Loss: 0.2534\n",
            "Epoch [9/10], Step [73/97], Loss: 0.3297\n",
            "Epoch [9/10], Step [74/97], Loss: 0.1447\n",
            "Epoch [9/10], Step [75/97], Loss: 0.1958\n",
            "Epoch [9/10], Step [76/97], Loss: 0.0821\n",
            "Epoch [9/10], Step [77/97], Loss: 0.2166\n",
            "Epoch [9/10], Step [78/97], Loss: 0.3151\n",
            "Epoch [9/10], Step [79/97], Loss: 0.3626\n",
            "Epoch [9/10], Step [80/97], Loss: 0.1818\n",
            "Epoch [9/10], Step [81/97], Loss: 0.1804\n",
            "Epoch [9/10], Step [82/97], Loss: 0.3024\n",
            "Epoch [9/10], Step [83/97], Loss: 0.2212\n",
            "Epoch [9/10], Step [84/97], Loss: 0.1357\n",
            "Epoch [9/10], Step [85/97], Loss: 0.1708\n",
            "Epoch [9/10], Step [86/97], Loss: 0.4031\n",
            "Epoch [9/10], Step [87/97], Loss: 0.0836\n",
            "Epoch [9/10], Step [88/97], Loss: 0.2188\n",
            "Epoch [9/10], Step [89/97], Loss: 0.2091\n",
            "Epoch [9/10], Step [90/97], Loss: 0.1778\n",
            "Epoch [9/10], Step [91/97], Loss: 0.1577\n",
            "Epoch [9/10], Step [92/97], Loss: 0.1488\n",
            "Epoch [9/10], Step [93/97], Loss: 0.1664\n",
            "Epoch [9/10], Step [94/97], Loss: 0.2437\n",
            "Epoch [9/10], Step [95/97], Loss: 0.1103\n",
            "Epoch [9/10], Step [96/97], Loss: 0.3202\n",
            "Epoch [9/10], Step [97/97], Loss: 0.0958\n",
            "Epoch finished\n",
            "Epoch [10/10], Step [1/97], Loss: 0.1634\n",
            "Epoch [10/10], Step [2/97], Loss: 0.1741\n",
            "Epoch [10/10], Step [3/97], Loss: 0.1561\n",
            "Epoch [10/10], Step [4/97], Loss: 0.2534\n",
            "Epoch [10/10], Step [5/97], Loss: 0.2041\n",
            "Epoch [10/10], Step [6/97], Loss: 0.2508\n",
            "Epoch [10/10], Step [7/97], Loss: 0.2984\n",
            "Epoch [10/10], Step [8/97], Loss: 0.1018\n",
            "Epoch [10/10], Step [9/97], Loss: 0.1725\n",
            "Epoch [10/10], Step [10/97], Loss: 0.0960\n",
            "Epoch [10/10], Step [11/97], Loss: 0.2447\n",
            "Epoch [10/10], Step [12/97], Loss: 0.2446\n",
            "Epoch [10/10], Step [13/97], Loss: 0.1902\n",
            "Epoch [10/10], Step [14/97], Loss: 0.1585\n",
            "Epoch [10/10], Step [15/97], Loss: 0.2626\n",
            "Epoch [10/10], Step [16/97], Loss: 0.1397\n",
            "Epoch [10/10], Step [17/97], Loss: 0.1651\n",
            "Epoch [10/10], Step [18/97], Loss: 0.2375\n",
            "Epoch [10/10], Step [19/97], Loss: 0.2601\n",
            "Epoch [10/10], Step [20/97], Loss: 0.1387\n",
            "Epoch [10/10], Step [21/97], Loss: 0.3625\n",
            "Epoch [10/10], Step [22/97], Loss: 0.2574\n",
            "Epoch [10/10], Step [23/97], Loss: 0.0490\n",
            "Epoch [10/10], Step [24/97], Loss: 0.1326\n",
            "Epoch [10/10], Step [25/97], Loss: 0.1070\n",
            "Epoch [10/10], Step [26/97], Loss: 0.2765\n",
            "Epoch [10/10], Step [27/97], Loss: 0.1513\n",
            "Epoch [10/10], Step [28/97], Loss: 0.2403\n",
            "Epoch [10/10], Step [29/97], Loss: 0.2430\n",
            "Epoch [10/10], Step [30/97], Loss: 0.4602\n",
            "Epoch [10/10], Step [31/97], Loss: 0.0820\n",
            "Epoch [10/10], Step [32/97], Loss: 0.1597\n",
            "Epoch [10/10], Step [33/97], Loss: 0.1712\n",
            "Epoch [10/10], Step [34/97], Loss: 0.1568\n",
            "Epoch [10/10], Step [35/97], Loss: 0.2364\n",
            "Epoch [10/10], Step [36/97], Loss: 0.2670\n",
            "Epoch [10/10], Step [37/97], Loss: 0.0768\n",
            "Epoch [10/10], Step [38/97], Loss: 0.0939\n",
            "Epoch [10/10], Step [39/97], Loss: 0.2797\n",
            "Epoch [10/10], Step [40/97], Loss: 0.2460\n",
            "Epoch [10/10], Step [41/97], Loss: 0.2431\n",
            "Epoch [10/10], Step [42/97], Loss: 0.1651\n",
            "Epoch [10/10], Step [43/97], Loss: 0.2104\n",
            "Epoch [10/10], Step [44/97], Loss: 0.1263\n",
            "Epoch [10/10], Step [45/97], Loss: 0.2200\n",
            "Epoch [10/10], Step [46/97], Loss: 0.1359\n",
            "Epoch [10/10], Step [47/97], Loss: 0.1623\n",
            "Epoch [10/10], Step [48/97], Loss: 0.2314\n",
            "Epoch [10/10], Step [49/97], Loss: 0.1902\n",
            "Epoch [10/10], Step [50/97], Loss: 0.2706\n",
            "Epoch [10/10], Step [51/97], Loss: 0.2616\n",
            "Epoch [10/10], Step [52/97], Loss: 0.1621\n",
            "Epoch [10/10], Step [53/97], Loss: 0.2431\n",
            "Epoch [10/10], Step [54/97], Loss: 0.1216\n",
            "Epoch [10/10], Step [55/97], Loss: 0.0751\n",
            "Epoch [10/10], Step [56/97], Loss: 0.1948\n",
            "Epoch [10/10], Step [57/97], Loss: 0.2040\n",
            "Epoch [10/10], Step [58/97], Loss: 0.3094\n",
            "Epoch [10/10], Step [59/97], Loss: 0.1513\n",
            "Epoch [10/10], Step [60/97], Loss: 0.1510\n",
            "Epoch [10/10], Step [61/97], Loss: 0.1497\n",
            "Epoch [10/10], Step [62/97], Loss: 0.1869\n",
            "Epoch [10/10], Step [63/97], Loss: 0.1975\n",
            "Epoch [10/10], Step [64/97], Loss: 0.1512\n",
            "Epoch [10/10], Step [65/97], Loss: 0.4098\n",
            "Epoch [10/10], Step [66/97], Loss: 0.1265\n",
            "Epoch [10/10], Step [67/97], Loss: 0.3679\n",
            "Epoch [10/10], Step [68/97], Loss: 0.1194\n",
            "Epoch [10/10], Step [69/97], Loss: 0.2395\n",
            "Epoch [10/10], Step [70/97], Loss: 0.1325\n",
            "Epoch [10/10], Step [71/97], Loss: 0.1705\n",
            "Epoch [10/10], Step [72/97], Loss: 0.1470\n",
            "Epoch [10/10], Step [73/97], Loss: 0.3282\n",
            "Epoch [10/10], Step [74/97], Loss: 0.0905\n",
            "Epoch [10/10], Step [75/97], Loss: 0.1447\n",
            "Epoch [10/10], Step [76/97], Loss: 0.1210\n",
            "Epoch [10/10], Step [77/97], Loss: 0.2906\n",
            "Epoch [10/10], Step [78/97], Loss: 0.2229\n",
            "Epoch [10/10], Step [79/97], Loss: 0.1578\n",
            "Epoch [10/10], Step [80/97], Loss: 0.1573\n",
            "Epoch [10/10], Step [81/97], Loss: 0.1909\n",
            "Epoch [10/10], Step [82/97], Loss: 0.1579\n",
            "Epoch [10/10], Step [83/97], Loss: 0.2009\n",
            "Epoch [10/10], Step [84/97], Loss: 0.1986\n",
            "Epoch [10/10], Step [85/97], Loss: 0.1296\n",
            "Epoch [10/10], Step [86/97], Loss: 0.1862\n",
            "Epoch [10/10], Step [87/97], Loss: 0.0964\n",
            "Epoch [10/10], Step [88/97], Loss: 0.1258\n",
            "Epoch [10/10], Step [89/97], Loss: 0.2625\n",
            "Epoch [10/10], Step [90/97], Loss: 0.1562\n",
            "Epoch [10/10], Step [91/97], Loss: 0.1609\n",
            "Epoch [10/10], Step [92/97], Loss: 0.1734\n",
            "Epoch [10/10], Step [93/97], Loss: 0.2934\n",
            "Epoch [10/10], Step [94/97], Loss: 0.1822\n",
            "Epoch [10/10], Step [95/97], Loss: 0.1217\n",
            "Epoch [10/10], Step [96/97], Loss: 0.2997\n",
            "Epoch [10/10], Step [97/97], Loss: 0.0970\n",
            "Epoch finished\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxwF9sDNlTY_",
        "outputId": "813b2fe3-4cc5-4de9-c62b-c7c62f47f83f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 88 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alex net using transfer learning\n",
        "alex= models.alexnet(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auJyG0l9tuuZ",
        "outputId": "46e47f24-87b1-4288-aefa-d9ea2f59d951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 215MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freez the model\n",
        "for param in alex.parameters():\n",
        "    param.requires_grad=False"
      ],
      "metadata": {
        "id": "51U8zdfJt0Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "alex.classifier=nn.Sequential(  nn.Linear(9216,1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=0.5),\n",
        "                                nn.Linear(1024,len(classes)),\n",
        "                                nn.LogSoftmax(dim=1))"
      ],
      "metadata": {
        "id": "GM1rZjBmt2Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alex = alex.to(device)\n",
        "alex.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTPfzoKiQ9ct",
        "outputId": "17fc960a-fa68-4fd4-d561-aaa2e386d3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=9216, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=1024, out_features=8, bias=True)\n",
              "    (4): LogSoftmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(alex.classifier.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "UKr7Y-1Ft5h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "total_step = len(train_loader)\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = alex(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    print('Epoch finished')\n",
        "\n",
        "print('Training finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AArtq47zt9Da",
        "outputId": "14360ccf-6243-46f3-a7ea-37bdbbc48c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [1/97], Loss: 2.0292\n",
            "Epoch [1/10], Step [2/97], Loss: 1.5469\n",
            "Epoch [1/10], Step [3/97], Loss: 1.5948\n",
            "Epoch [1/10], Step [4/97], Loss: 1.4515\n",
            "Epoch [1/10], Step [5/97], Loss: 0.8846\n",
            "Epoch [1/10], Step [6/97], Loss: 0.5577\n",
            "Epoch [1/10], Step [7/97], Loss: 0.7313\n",
            "Epoch [1/10], Step [8/97], Loss: 0.1202\n",
            "Epoch [1/10], Step [9/97], Loss: 0.3494\n",
            "Epoch [1/10], Step [10/97], Loss: 0.0830\n",
            "Epoch [1/10], Step [11/97], Loss: 0.4708\n",
            "Epoch [1/10], Step [12/97], Loss: 0.1561\n",
            "Epoch [1/10], Step [13/97], Loss: 0.3583\n",
            "Epoch [1/10], Step [14/97], Loss: 0.4467\n",
            "Epoch [1/10], Step [15/97], Loss: 0.2634\n",
            "Epoch [1/10], Step [16/97], Loss: 0.4821\n",
            "Epoch [1/10], Step [17/97], Loss: 0.1102\n",
            "Epoch [1/10], Step [18/97], Loss: 0.7718\n",
            "Epoch [1/10], Step [19/97], Loss: 0.4762\n",
            "Epoch [1/10], Step [20/97], Loss: 0.7372\n",
            "Epoch [1/10], Step [21/97], Loss: 0.3005\n",
            "Epoch [1/10], Step [22/97], Loss: 0.0775\n",
            "Epoch [1/10], Step [23/97], Loss: 0.4429\n",
            "Epoch [1/10], Step [24/97], Loss: 1.1583\n",
            "Epoch [1/10], Step [25/97], Loss: 0.4618\n",
            "Epoch [1/10], Step [26/97], Loss: 0.0641\n",
            "Epoch [1/10], Step [27/97], Loss: 0.0006\n",
            "Epoch [1/10], Step [28/97], Loss: 0.1441\n",
            "Epoch [1/10], Step [29/97], Loss: 0.2751\n",
            "Epoch [1/10], Step [30/97], Loss: 0.5391\n",
            "Epoch [1/10], Step [31/97], Loss: 0.1627\n",
            "Epoch [1/10], Step [32/97], Loss: 0.0452\n",
            "Epoch [1/10], Step [33/97], Loss: 0.2166\n",
            "Epoch [1/10], Step [34/97], Loss: 0.9447\n",
            "Epoch [1/10], Step [35/97], Loss: 0.6129\n",
            "Epoch [1/10], Step [36/97], Loss: 0.4585\n",
            "Epoch [1/10], Step [37/97], Loss: 0.0822\n",
            "Epoch [1/10], Step [38/97], Loss: 0.2490\n",
            "Epoch [1/10], Step [39/97], Loss: 0.1897\n",
            "Epoch [1/10], Step [40/97], Loss: 0.1878\n",
            "Epoch [1/10], Step [41/97], Loss: 0.3172\n",
            "Epoch [1/10], Step [42/97], Loss: 1.0217\n",
            "Epoch [1/10], Step [43/97], Loss: 0.0381\n",
            "Epoch [1/10], Step [44/97], Loss: 0.1611\n",
            "Epoch [1/10], Step [45/97], Loss: 0.3591\n",
            "Epoch [1/10], Step [46/97], Loss: 0.2331\n",
            "Epoch [1/10], Step [47/97], Loss: 0.0946\n",
            "Epoch [1/10], Step [48/97], Loss: 0.2270\n",
            "Epoch [1/10], Step [49/97], Loss: 0.0707\n",
            "Epoch [1/10], Step [50/97], Loss: 0.0006\n",
            "Epoch [1/10], Step [51/97], Loss: 0.2776\n",
            "Epoch [1/10], Step [52/97], Loss: 0.2826\n",
            "Epoch [1/10], Step [53/97], Loss: 0.1845\n",
            "Epoch [1/10], Step [54/97], Loss: 0.2012\n",
            "Epoch [1/10], Step [55/97], Loss: 0.0968\n",
            "Epoch [1/10], Step [56/97], Loss: 0.0068\n",
            "Epoch [1/10], Step [57/97], Loss: 0.2779\n",
            "Epoch [1/10], Step [58/97], Loss: 0.0974\n",
            "Epoch [1/10], Step [59/97], Loss: 0.0307\n",
            "Epoch [1/10], Step [60/97], Loss: 0.1375\n",
            "Epoch [1/10], Step [61/97], Loss: 0.1003\n",
            "Epoch [1/10], Step [62/97], Loss: 0.0044\n",
            "Epoch [1/10], Step [63/97], Loss: 0.0096\n",
            "Epoch [1/10], Step [64/97], Loss: 0.1030\n",
            "Epoch [1/10], Step [65/97], Loss: 0.3250\n",
            "Epoch [1/10], Step [66/97], Loss: 0.2371\n",
            "Epoch [1/10], Step [67/97], Loss: 0.4347\n",
            "Epoch [1/10], Step [68/97], Loss: 0.2097\n",
            "Epoch [1/10], Step [69/97], Loss: 0.5094\n",
            "Epoch [1/10], Step [70/97], Loss: 0.0007\n",
            "Epoch [1/10], Step [71/97], Loss: 0.0959\n",
            "Epoch [1/10], Step [72/97], Loss: 0.0308\n",
            "Epoch [1/10], Step [73/97], Loss: 0.3044\n",
            "Epoch [1/10], Step [74/97], Loss: 0.1449\n",
            "Epoch [1/10], Step [75/97], Loss: 0.1759\n",
            "Epoch [1/10], Step [76/97], Loss: 0.0345\n",
            "Epoch [1/10], Step [77/97], Loss: 0.1104\n",
            "Epoch [1/10], Step [78/97], Loss: 0.2230\n",
            "Epoch [1/10], Step [79/97], Loss: 0.1121\n",
            "Epoch [1/10], Step [80/97], Loss: 0.1900\n",
            "Epoch [1/10], Step [81/97], Loss: 0.0206\n",
            "Epoch [1/10], Step [82/97], Loss: 0.2664\n",
            "Epoch [1/10], Step [83/97], Loss: 0.3605\n",
            "Epoch [1/10], Step [84/97], Loss: 0.2315\n",
            "Epoch [1/10], Step [85/97], Loss: 0.0812\n",
            "Epoch [1/10], Step [86/97], Loss: 0.0658\n",
            "Epoch [1/10], Step [87/97], Loss: 0.2311\n",
            "Epoch [1/10], Step [88/97], Loss: 0.0271\n",
            "Epoch [1/10], Step [89/97], Loss: 0.2447\n",
            "Epoch [1/10], Step [90/97], Loss: 0.5152\n",
            "Epoch [1/10], Step [91/97], Loss: 0.1297\n",
            "Epoch [1/10], Step [92/97], Loss: 0.2013\n",
            "Epoch [1/10], Step [93/97], Loss: 0.2957\n",
            "Epoch [1/10], Step [94/97], Loss: 0.6283\n",
            "Epoch [1/10], Step [95/97], Loss: 0.0076\n",
            "Epoch [1/10], Step [96/97], Loss: 0.1036\n",
            "Epoch [1/10], Step [97/97], Loss: 0.0166\n",
            "Epoch finished\n",
            "Epoch [2/10], Step [1/97], Loss: 0.1364\n",
            "Epoch [2/10], Step [2/97], Loss: 0.1056\n",
            "Epoch [2/10], Step [3/97], Loss: 0.0260\n",
            "Epoch [2/10], Step [4/97], Loss: 0.0234\n",
            "Epoch [2/10], Step [5/97], Loss: 0.1625\n",
            "Epoch [2/10], Step [6/97], Loss: 0.3772\n",
            "Epoch [2/10], Step [7/97], Loss: 0.2949\n",
            "Epoch [2/10], Step [8/97], Loss: 0.0168\n",
            "Epoch [2/10], Step [9/97], Loss: 0.1451\n",
            "Epoch [2/10], Step [10/97], Loss: 0.1117\n",
            "Epoch [2/10], Step [11/97], Loss: 0.1246\n",
            "Epoch [2/10], Step [12/97], Loss: 0.0758\n",
            "Epoch [2/10], Step [13/97], Loss: 0.1805\n",
            "Epoch [2/10], Step [14/97], Loss: 0.1339\n",
            "Epoch [2/10], Step [15/97], Loss: 0.2232\n",
            "Epoch [2/10], Step [16/97], Loss: 0.0072\n",
            "Epoch [2/10], Step [17/97], Loss: 0.0665\n",
            "Epoch [2/10], Step [18/97], Loss: 0.2012\n",
            "Epoch [2/10], Step [19/97], Loss: 0.2123\n",
            "Epoch [2/10], Step [20/97], Loss: 0.0432\n",
            "Epoch [2/10], Step [21/97], Loss: 0.3631\n",
            "Epoch [2/10], Step [22/97], Loss: 0.1352\n",
            "Epoch [2/10], Step [23/97], Loss: 0.0124\n",
            "Epoch [2/10], Step [24/97], Loss: 0.0252\n",
            "Epoch [2/10], Step [25/97], Loss: 0.0733\n",
            "Epoch [2/10], Step [26/97], Loss: 0.1029\n",
            "Epoch [2/10], Step [27/97], Loss: 0.0520\n",
            "Epoch [2/10], Step [28/97], Loss: 0.0453\n",
            "Epoch [2/10], Step [29/97], Loss: 0.1464\n",
            "Epoch [2/10], Step [30/97], Loss: 0.0829\n",
            "Epoch [2/10], Step [31/97], Loss: 0.0653\n",
            "Epoch [2/10], Step [32/97], Loss: 0.0488\n",
            "Epoch [2/10], Step [33/97], Loss: 0.1431\n",
            "Epoch [2/10], Step [34/97], Loss: 0.0872\n",
            "Epoch [2/10], Step [35/97], Loss: 0.1423\n",
            "Epoch [2/10], Step [36/97], Loss: 0.0041\n",
            "Epoch [2/10], Step [37/97], Loss: 0.0012\n",
            "Epoch [2/10], Step [38/97], Loss: 0.0364\n",
            "Epoch [2/10], Step [39/97], Loss: 0.0153\n",
            "Epoch [2/10], Step [40/97], Loss: 0.3688\n",
            "Epoch [2/10], Step [41/97], Loss: 0.0116\n",
            "Epoch [2/10], Step [42/97], Loss: 0.1299\n",
            "Epoch [2/10], Step [43/97], Loss: 0.0038\n",
            "Epoch [2/10], Step [44/97], Loss: 0.0110\n",
            "Epoch [2/10], Step [45/97], Loss: 0.0956\n",
            "Epoch [2/10], Step [46/97], Loss: 0.0049\n",
            "Epoch [2/10], Step [47/97], Loss: 0.0135\n",
            "Epoch [2/10], Step [48/97], Loss: 0.1830\n",
            "Epoch [2/10], Step [49/97], Loss: 0.0329\n",
            "Epoch [2/10], Step [50/97], Loss: 0.2039\n",
            "Epoch [2/10], Step [51/97], Loss: 0.1059\n",
            "Epoch [2/10], Step [52/97], Loss: 0.1157\n",
            "Epoch [2/10], Step [53/97], Loss: 0.3328\n",
            "Epoch [2/10], Step [54/97], Loss: 0.5375\n",
            "Epoch [2/10], Step [55/97], Loss: 0.0939\n",
            "Epoch [2/10], Step [56/97], Loss: 0.0150\n",
            "Epoch [2/10], Step [57/97], Loss: 0.1007\n",
            "Epoch [2/10], Step [58/97], Loss: 0.0046\n",
            "Epoch [2/10], Step [59/97], Loss: 0.1439\n",
            "Epoch [2/10], Step [60/97], Loss: 0.0095\n",
            "Epoch [2/10], Step [61/97], Loss: 0.0899\n",
            "Epoch [2/10], Step [62/97], Loss: 0.0733\n",
            "Epoch [2/10], Step [63/97], Loss: 0.2986\n",
            "Epoch [2/10], Step [64/97], Loss: 0.2323\n",
            "Epoch [2/10], Step [65/97], Loss: 0.3447\n",
            "Epoch [2/10], Step [66/97], Loss: 0.0944\n",
            "Epoch [2/10], Step [67/97], Loss: 0.1371\n",
            "Epoch [2/10], Step [68/97], Loss: 0.1245\n",
            "Epoch [2/10], Step [69/97], Loss: 0.0137\n",
            "Epoch [2/10], Step [70/97], Loss: 0.3711\n",
            "Epoch [2/10], Step [71/97], Loss: 0.0158\n",
            "Epoch [2/10], Step [72/97], Loss: 0.1269\n",
            "Epoch [2/10], Step [73/97], Loss: 0.2448\n",
            "Epoch [2/10], Step [74/97], Loss: 0.0247\n",
            "Epoch [2/10], Step [75/97], Loss: 0.1858\n",
            "Epoch [2/10], Step [76/97], Loss: 0.0816\n",
            "Epoch [2/10], Step [77/97], Loss: 0.0059\n",
            "Epoch [2/10], Step [78/97], Loss: 0.3899\n",
            "Epoch [2/10], Step [79/97], Loss: 0.0229\n",
            "Epoch [2/10], Step [80/97], Loss: 0.1043\n",
            "Epoch [2/10], Step [81/97], Loss: 0.0841\n",
            "Epoch [2/10], Step [82/97], Loss: 0.0137\n",
            "Epoch [2/10], Step [83/97], Loss: 0.1087\n",
            "Epoch [2/10], Step [84/97], Loss: 0.1539\n",
            "Epoch [2/10], Step [85/97], Loss: 0.1191\n",
            "Epoch [2/10], Step [86/97], Loss: 0.0540\n",
            "Epoch [2/10], Step [87/97], Loss: 0.0089\n",
            "Epoch [2/10], Step [88/97], Loss: 0.0545\n",
            "Epoch [2/10], Step [89/97], Loss: 0.1139\n",
            "Epoch [2/10], Step [90/97], Loss: 0.4100\n",
            "Epoch [2/10], Step [91/97], Loss: 0.0123\n",
            "Epoch [2/10], Step [92/97], Loss: 0.1860\n",
            "Epoch [2/10], Step [93/97], Loss: 0.4223\n",
            "Epoch [2/10], Step [94/97], Loss: 0.2781\n",
            "Epoch [2/10], Step [95/97], Loss: 0.0763\n",
            "Epoch [2/10], Step [96/97], Loss: 0.0162\n",
            "Epoch [2/10], Step [97/97], Loss: 0.0004\n",
            "Epoch finished\n",
            "Epoch [3/10], Step [1/97], Loss: 0.0068\n",
            "Epoch [3/10], Step [2/97], Loss: 0.1018\n",
            "Epoch [3/10], Step [3/97], Loss: 0.1985\n",
            "Epoch [3/10], Step [4/97], Loss: 0.0180\n",
            "Epoch [3/10], Step [5/97], Loss: 0.0014\n",
            "Epoch [3/10], Step [6/97], Loss: 0.1671\n",
            "Epoch [3/10], Step [7/97], Loss: 0.1463\n",
            "Epoch [3/10], Step [8/97], Loss: 0.0700\n",
            "Epoch [3/10], Step [9/97], Loss: 0.2254\n",
            "Epoch [3/10], Step [10/97], Loss: 0.3047\n",
            "Epoch [3/10], Step [11/97], Loss: 0.2167\n",
            "Epoch [3/10], Step [12/97], Loss: 0.2305\n",
            "Epoch [3/10], Step [13/97], Loss: 0.1129\n",
            "Epoch [3/10], Step [14/97], Loss: 0.0260\n",
            "Epoch [3/10], Step [15/97], Loss: 0.1275\n",
            "Epoch [3/10], Step [16/97], Loss: 0.0276\n",
            "Epoch [3/10], Step [17/97], Loss: 0.0355\n",
            "Epoch [3/10], Step [18/97], Loss: 0.0399\n",
            "Epoch [3/10], Step [19/97], Loss: 0.0735\n",
            "Epoch [3/10], Step [20/97], Loss: 0.2253\n",
            "Epoch [3/10], Step [21/97], Loss: 0.0911\n",
            "Epoch [3/10], Step [22/97], Loss: 0.1315\n",
            "Epoch [3/10], Step [23/97], Loss: 0.0020\n",
            "Epoch [3/10], Step [24/97], Loss: 0.3135\n",
            "Epoch [3/10], Step [25/97], Loss: 0.0106\n",
            "Epoch [3/10], Step [26/97], Loss: 0.0222\n",
            "Epoch [3/10], Step [27/97], Loss: 0.0315\n",
            "Epoch [3/10], Step [28/97], Loss: 0.0526\n",
            "Epoch [3/10], Step [29/97], Loss: 0.0184\n",
            "Epoch [3/10], Step [30/97], Loss: 0.0797\n",
            "Epoch [3/10], Step [31/97], Loss: 0.2525\n",
            "Epoch [3/10], Step [32/97], Loss: 0.1756\n",
            "Epoch [3/10], Step [33/97], Loss: 0.1766\n",
            "Epoch [3/10], Step [34/97], Loss: 0.1787\n",
            "Epoch [3/10], Step [35/97], Loss: 0.0546\n",
            "Epoch [3/10], Step [36/97], Loss: 0.0732\n",
            "Epoch [3/10], Step [37/97], Loss: 0.0041\n",
            "Epoch [3/10], Step [38/97], Loss: 0.0783\n",
            "Epoch [3/10], Step [39/97], Loss: 0.1035\n",
            "Epoch [3/10], Step [40/97], Loss: 0.0618\n",
            "Epoch [3/10], Step [41/97], Loss: 0.0808\n",
            "Epoch [3/10], Step [42/97], Loss: 0.1684\n",
            "Epoch [3/10], Step [43/97], Loss: 0.0374\n",
            "Epoch [3/10], Step [44/97], Loss: 0.0870\n",
            "Epoch [3/10], Step [45/97], Loss: 0.0685\n",
            "Epoch [3/10], Step [46/97], Loss: 0.0160\n",
            "Epoch [3/10], Step [47/97], Loss: 0.0068\n",
            "Epoch [3/10], Step [48/97], Loss: 0.1375\n",
            "Epoch [3/10], Step [49/97], Loss: 0.0434\n",
            "Epoch [3/10], Step [50/97], Loss: 0.0372\n",
            "Epoch [3/10], Step [51/97], Loss: 0.1587\n",
            "Epoch [3/10], Step [52/97], Loss: 0.0148\n",
            "Epoch [3/10], Step [53/97], Loss: 0.0663\n",
            "Epoch [3/10], Step [54/97], Loss: 0.0556\n",
            "Epoch [3/10], Step [55/97], Loss: 0.0949\n",
            "Epoch [3/10], Step [56/97], Loss: 0.0010\n",
            "Epoch [3/10], Step [57/97], Loss: 0.2141\n",
            "Epoch [3/10], Step [58/97], Loss: 0.0168\n",
            "Epoch [3/10], Step [59/97], Loss: 0.0127\n",
            "Epoch [3/10], Step [60/97], Loss: 0.0815\n",
            "Epoch [3/10], Step [61/97], Loss: 0.0254\n",
            "Epoch [3/10], Step [62/97], Loss: 0.0055\n",
            "Epoch [3/10], Step [63/97], Loss: 0.0768\n",
            "Epoch [3/10], Step [64/97], Loss: 0.0519\n",
            "Epoch [3/10], Step [65/97], Loss: 0.1108\n",
            "Epoch [3/10], Step [66/97], Loss: 0.1226\n",
            "Epoch [3/10], Step [67/97], Loss: 0.1429\n",
            "Epoch [3/10], Step [68/97], Loss: 0.0862\n",
            "Epoch [3/10], Step [69/97], Loss: 0.0756\n",
            "Epoch [3/10], Step [70/97], Loss: 0.0994\n",
            "Epoch [3/10], Step [71/97], Loss: 0.0085\n",
            "Epoch [3/10], Step [72/97], Loss: 0.0570\n",
            "Epoch [3/10], Step [73/97], Loss: 0.3262\n",
            "Epoch [3/10], Step [74/97], Loss: 0.0003\n",
            "Epoch [3/10], Step [75/97], Loss: 0.0105\n",
            "Epoch [3/10], Step [76/97], Loss: 0.0271\n",
            "Epoch [3/10], Step [77/97], Loss: 0.0595\n",
            "Epoch [3/10], Step [78/97], Loss: 0.1966\n",
            "Epoch [3/10], Step [79/97], Loss: 0.0308\n",
            "Epoch [3/10], Step [80/97], Loss: 0.0584\n",
            "Epoch [3/10], Step [81/97], Loss: 0.0023\n",
            "Epoch [3/10], Step [82/97], Loss: 0.0102\n",
            "Epoch [3/10], Step [83/97], Loss: 0.2097\n",
            "Epoch [3/10], Step [84/97], Loss: 0.1467\n",
            "Epoch [3/10], Step [85/97], Loss: 0.0348\n",
            "Epoch [3/10], Step [86/97], Loss: 0.1671\n",
            "Epoch [3/10], Step [87/97], Loss: 0.0056\n",
            "Epoch [3/10], Step [88/97], Loss: 0.0570\n",
            "Epoch [3/10], Step [89/97], Loss: 0.3375\n",
            "Epoch [3/10], Step [90/97], Loss: 0.4286\n",
            "Epoch [3/10], Step [91/97], Loss: 0.1414\n",
            "Epoch [3/10], Step [92/97], Loss: 0.0129\n",
            "Epoch [3/10], Step [93/97], Loss: 0.2235\n",
            "Epoch [3/10], Step [94/97], Loss: 0.6898\n",
            "Epoch [3/10], Step [95/97], Loss: 0.0222\n",
            "Epoch [3/10], Step [96/97], Loss: 0.2462\n",
            "Epoch [3/10], Step [97/97], Loss: 0.0901\n",
            "Epoch finished\n",
            "Epoch [4/10], Step [1/97], Loss: 0.1047\n",
            "Epoch [4/10], Step [2/97], Loss: 0.1695\n",
            "Epoch [4/10], Step [3/97], Loss: 0.1932\n",
            "Epoch [4/10], Step [4/97], Loss: 0.4160\n",
            "Epoch [4/10], Step [5/97], Loss: 0.1152\n",
            "Epoch [4/10], Step [6/97], Loss: 0.1963\n",
            "Epoch [4/10], Step [7/97], Loss: 0.4904\n",
            "Epoch [4/10], Step [8/97], Loss: 0.0018\n",
            "Epoch [4/10], Step [9/97], Loss: 0.0599\n",
            "Epoch [4/10], Step [10/97], Loss: 0.0318\n",
            "Epoch [4/10], Step [11/97], Loss: 0.0911\n",
            "Epoch [4/10], Step [12/97], Loss: 0.2152\n",
            "Epoch [4/10], Step [13/97], Loss: 0.1042\n",
            "Epoch [4/10], Step [14/97], Loss: 0.1300\n",
            "Epoch [4/10], Step [15/97], Loss: 0.0700\n",
            "Epoch [4/10], Step [16/97], Loss: 0.0025\n",
            "Epoch [4/10], Step [17/97], Loss: 0.0041\n",
            "Epoch [4/10], Step [18/97], Loss: 0.0673\n",
            "Epoch [4/10], Step [19/97], Loss: 0.0553\n",
            "Epoch [4/10], Step [20/97], Loss: 0.0341\n",
            "Epoch [4/10], Step [21/97], Loss: 0.0767\n",
            "Epoch [4/10], Step [22/97], Loss: 0.3474\n",
            "Epoch [4/10], Step [23/97], Loss: 0.0004\n",
            "Epoch [4/10], Step [24/97], Loss: 0.0122\n",
            "Epoch [4/10], Step [25/97], Loss: 0.0442\n",
            "Epoch [4/10], Step [26/97], Loss: 0.0023\n",
            "Epoch [4/10], Step [27/97], Loss: 0.0058\n",
            "Epoch [4/10], Step [28/97], Loss: 0.0009\n",
            "Epoch [4/10], Step [29/97], Loss: 0.0829\n",
            "Epoch [4/10], Step [30/97], Loss: 0.1546\n",
            "Epoch [4/10], Step [31/97], Loss: 0.0037\n",
            "Epoch [4/10], Step [32/97], Loss: 0.0418\n",
            "Epoch [4/10], Step [33/97], Loss: 0.2186\n",
            "Epoch [4/10], Step [34/97], Loss: 0.0529\n",
            "Epoch [4/10], Step [35/97], Loss: 0.0070\n",
            "Epoch [4/10], Step [36/97], Loss: 0.1640\n",
            "Epoch [4/10], Step [37/97], Loss: 0.0143\n",
            "Epoch [4/10], Step [38/97], Loss: 0.0216\n",
            "Epoch [4/10], Step [39/97], Loss: 0.1249\n",
            "Epoch [4/10], Step [40/97], Loss: 0.0008\n",
            "Epoch [4/10], Step [41/97], Loss: 0.0548\n",
            "Epoch [4/10], Step [42/97], Loss: 0.1557\n",
            "Epoch [4/10], Step [43/97], Loss: 0.0353\n",
            "Epoch [4/10], Step [44/97], Loss: 0.0142\n",
            "Epoch [4/10], Step [45/97], Loss: 0.1104\n",
            "Epoch [4/10], Step [46/97], Loss: 0.1300\n",
            "Epoch [4/10], Step [47/97], Loss: 0.0632\n",
            "Epoch [4/10], Step [48/97], Loss: 0.0128\n",
            "Epoch [4/10], Step [49/97], Loss: 0.0048\n",
            "Epoch [4/10], Step [50/97], Loss: 0.0256\n",
            "Epoch [4/10], Step [51/97], Loss: 0.0242\n",
            "Epoch [4/10], Step [52/97], Loss: 0.0453\n",
            "Epoch [4/10], Step [53/97], Loss: 0.0659\n",
            "Epoch [4/10], Step [54/97], Loss: 0.0105\n",
            "Epoch [4/10], Step [55/97], Loss: 0.0018\n",
            "Epoch [4/10], Step [56/97], Loss: 0.0367\n",
            "Epoch [4/10], Step [57/97], Loss: 0.1298\n",
            "Epoch [4/10], Step [58/97], Loss: 0.0120\n",
            "Epoch [4/10], Step [59/97], Loss: 0.0334\n",
            "Epoch [4/10], Step [60/97], Loss: 0.1140\n",
            "Epoch [4/10], Step [61/97], Loss: 0.0735\n",
            "Epoch [4/10], Step [62/97], Loss: 0.0006\n",
            "Epoch [4/10], Step [63/97], Loss: 0.0131\n",
            "Epoch [4/10], Step [64/97], Loss: 0.1338\n",
            "Epoch [4/10], Step [65/97], Loss: 0.0795\n",
            "Epoch [4/10], Step [66/97], Loss: 0.1067\n",
            "Epoch [4/10], Step [67/97], Loss: 0.1805\n",
            "Epoch [4/10], Step [68/97], Loss: 0.0760\n",
            "Epoch [4/10], Step [69/97], Loss: 0.0368\n",
            "Epoch [4/10], Step [70/97], Loss: 0.0779\n",
            "Epoch [4/10], Step [71/97], Loss: 0.0116\n",
            "Epoch [4/10], Step [72/97], Loss: 0.0287\n",
            "Epoch [4/10], Step [73/97], Loss: 0.0240\n",
            "Epoch [4/10], Step [74/97], Loss: 0.0316\n",
            "Epoch [4/10], Step [75/97], Loss: 0.1320\n",
            "Epoch [4/10], Step [76/97], Loss: 0.0389\n",
            "Epoch [4/10], Step [77/97], Loss: 0.0122\n",
            "Epoch [4/10], Step [78/97], Loss: 0.1714\n",
            "Epoch [4/10], Step [79/97], Loss: 0.3535\n",
            "Epoch [4/10], Step [80/97], Loss: 0.0238\n",
            "Epoch [4/10], Step [81/97], Loss: 0.0265\n",
            "Epoch [4/10], Step [82/97], Loss: 0.1845\n",
            "Epoch [4/10], Step [83/97], Loss: 0.0582\n",
            "Epoch [4/10], Step [84/97], Loss: 0.0178\n",
            "Epoch [4/10], Step [85/97], Loss: 0.0022\n",
            "Epoch [4/10], Step [86/97], Loss: 0.1642\n",
            "Epoch [4/10], Step [87/97], Loss: 0.1225\n",
            "Epoch [4/10], Step [88/97], Loss: 0.0122\n",
            "Epoch [4/10], Step [89/97], Loss: 0.0472\n",
            "Epoch [4/10], Step [90/97], Loss: 0.1819\n",
            "Epoch [4/10], Step [91/97], Loss: 0.0020\n",
            "Epoch [4/10], Step [92/97], Loss: 0.0735\n",
            "Epoch [4/10], Step [93/97], Loss: 0.0165\n",
            "Epoch [4/10], Step [94/97], Loss: 0.1736\n",
            "Epoch [4/10], Step [95/97], Loss: 0.0156\n",
            "Epoch [4/10], Step [96/97], Loss: 0.0128\n",
            "Epoch [4/10], Step [97/97], Loss: 0.0005\n",
            "Epoch finished\n",
            "Epoch [5/10], Step [1/97], Loss: 0.0285\n",
            "Epoch [5/10], Step [2/97], Loss: 0.0229\n",
            "Epoch [5/10], Step [3/97], Loss: 0.0220\n",
            "Epoch [5/10], Step [4/97], Loss: 0.0005\n",
            "Epoch [5/10], Step [5/97], Loss: 0.0029\n",
            "Epoch [5/10], Step [6/97], Loss: 0.0154\n",
            "Epoch [5/10], Step [7/97], Loss: 0.0439\n",
            "Epoch [5/10], Step [8/97], Loss: 0.0106\n",
            "Epoch [5/10], Step [9/97], Loss: 0.1421\n",
            "Epoch [5/10], Step [10/97], Loss: 0.0159\n",
            "Epoch [5/10], Step [11/97], Loss: 0.1340\n",
            "Epoch [5/10], Step [12/97], Loss: 0.2378\n",
            "Epoch [5/10], Step [13/97], Loss: 0.0219\n",
            "Epoch [5/10], Step [14/97], Loss: 0.0107\n",
            "Epoch [5/10], Step [15/97], Loss: 0.0088\n",
            "Epoch [5/10], Step [16/97], Loss: 0.1112\n",
            "Epoch [5/10], Step [17/97], Loss: 0.0113\n",
            "Epoch [5/10], Step [18/97], Loss: 0.0514\n",
            "Epoch [5/10], Step [19/97], Loss: 0.1888\n",
            "Epoch [5/10], Step [20/97], Loss: 0.0264\n",
            "Epoch [5/10], Step [21/97], Loss: 0.0895\n",
            "Epoch [5/10], Step [22/97], Loss: 0.2703\n",
            "Epoch [5/10], Step [23/97], Loss: 0.0008\n",
            "Epoch [5/10], Step [24/97], Loss: 0.1355\n",
            "Epoch [5/10], Step [25/97], Loss: 0.1302\n",
            "Epoch [5/10], Step [26/97], Loss: 0.0728\n",
            "Epoch [5/10], Step [27/97], Loss: 0.0034\n",
            "Epoch [5/10], Step [28/97], Loss: 0.3411\n",
            "Epoch [5/10], Step [29/97], Loss: 0.0618\n",
            "Epoch [5/10], Step [30/97], Loss: 0.0810\n",
            "Epoch [5/10], Step [31/97], Loss: 0.0434\n",
            "Epoch [5/10], Step [32/97], Loss: 0.3245\n",
            "Epoch [5/10], Step [33/97], Loss: 0.1966\n",
            "Epoch [5/10], Step [34/97], Loss: 0.0144\n",
            "Epoch [5/10], Step [35/97], Loss: 0.0383\n",
            "Epoch [5/10], Step [36/97], Loss: 0.1472\n",
            "Epoch [5/10], Step [37/97], Loss: 0.0058\n",
            "Epoch [5/10], Step [38/97], Loss: 0.0031\n",
            "Epoch [5/10], Step [39/97], Loss: 0.0297\n",
            "Epoch [5/10], Step [40/97], Loss: 0.2395\n",
            "Epoch [5/10], Step [41/97], Loss: 0.0266\n",
            "Epoch [5/10], Step [42/97], Loss: 0.1416\n",
            "Epoch [5/10], Step [43/97], Loss: 0.0871\n",
            "Epoch [5/10], Step [44/97], Loss: 0.0937\n",
            "Epoch [5/10], Step [45/97], Loss: 0.0619\n",
            "Epoch [5/10], Step [46/97], Loss: 0.0223\n",
            "Epoch [5/10], Step [47/97], Loss: 0.0185\n",
            "Epoch [5/10], Step [48/97], Loss: 0.1715\n",
            "Epoch [5/10], Step [49/97], Loss: 0.1557\n",
            "Epoch [5/10], Step [50/97], Loss: 0.0138\n",
            "Epoch [5/10], Step [51/97], Loss: 0.1076\n",
            "Epoch [5/10], Step [52/97], Loss: 0.0467\n",
            "Epoch [5/10], Step [53/97], Loss: 0.2426\n",
            "Epoch [5/10], Step [54/97], Loss: 0.0715\n",
            "Epoch [5/10], Step [55/97], Loss: 0.1071\n",
            "Epoch [5/10], Step [56/97], Loss: 0.1493\n",
            "Epoch [5/10], Step [57/97], Loss: 0.0713\n",
            "Epoch [5/10], Step [58/97], Loss: 0.0006\n",
            "Epoch [5/10], Step [59/97], Loss: 0.0411\n",
            "Epoch [5/10], Step [60/97], Loss: 0.0751\n",
            "Epoch [5/10], Step [61/97], Loss: 0.0370\n",
            "Epoch [5/10], Step [62/97], Loss: 0.1076\n",
            "Epoch [5/10], Step [63/97], Loss: 0.1428\n",
            "Epoch [5/10], Step [64/97], Loss: 0.0235\n",
            "Epoch [5/10], Step [65/97], Loss: 0.1247\n",
            "Epoch [5/10], Step [66/97], Loss: 0.0055\n",
            "Epoch [5/10], Step [67/97], Loss: 0.1773\n",
            "Epoch [5/10], Step [68/97], Loss: 0.0341\n",
            "Epoch [5/10], Step [69/97], Loss: 0.1405\n",
            "Epoch [5/10], Step [70/97], Loss: 0.0236\n",
            "Epoch [5/10], Step [71/97], Loss: 0.0989\n",
            "Epoch [5/10], Step [72/97], Loss: 0.0003\n",
            "Epoch [5/10], Step [73/97], Loss: 0.0578\n",
            "Epoch [5/10], Step [74/97], Loss: 0.0017\n",
            "Epoch [5/10], Step [75/97], Loss: 0.0134\n",
            "Epoch [5/10], Step [76/97], Loss: 0.0374\n",
            "Epoch [5/10], Step [77/97], Loss: 0.0446\n",
            "Epoch [5/10], Step [78/97], Loss: 0.1446\n",
            "Epoch [5/10], Step [79/97], Loss: 0.1263\n",
            "Epoch [5/10], Step [80/97], Loss: 0.0219\n",
            "Epoch [5/10], Step [81/97], Loss: 0.0049\n",
            "Epoch [5/10], Step [82/97], Loss: 0.0034\n",
            "Epoch [5/10], Step [83/97], Loss: 0.1148\n",
            "Epoch [5/10], Step [84/97], Loss: 0.0237\n",
            "Epoch [5/10], Step [85/97], Loss: 0.1217\n",
            "Epoch [5/10], Step [86/97], Loss: 0.2201\n",
            "Epoch [5/10], Step [87/97], Loss: 0.0368\n",
            "Epoch [5/10], Step [88/97], Loss: 0.1487\n",
            "Epoch [5/10], Step [89/97], Loss: 0.0658\n",
            "Epoch [5/10], Step [90/97], Loss: 0.0896\n",
            "Epoch [5/10], Step [91/97], Loss: 0.0974\n",
            "Epoch [5/10], Step [92/97], Loss: 0.0338\n",
            "Epoch [5/10], Step [93/97], Loss: 0.1286\n",
            "Epoch [5/10], Step [94/97], Loss: 0.1885\n",
            "Epoch [5/10], Step [95/97], Loss: 0.1004\n",
            "Epoch [5/10], Step [96/97], Loss: 0.1780\n",
            "Epoch [5/10], Step [97/97], Loss: 0.0226\n",
            "Epoch finished\n",
            "Epoch [6/10], Step [1/97], Loss: 0.0014\n",
            "Epoch [6/10], Step [2/97], Loss: 0.0023\n",
            "Epoch [6/10], Step [3/97], Loss: 0.1280\n",
            "Epoch [6/10], Step [4/97], Loss: 0.0605\n",
            "Epoch [6/10], Step [5/97], Loss: 0.1737\n",
            "Epoch [6/10], Step [6/97], Loss: 0.0833\n",
            "Epoch [6/10], Step [7/97], Loss: 0.1067\n",
            "Epoch [6/10], Step [8/97], Loss: 0.0942\n",
            "Epoch [6/10], Step [9/97], Loss: 0.0405\n",
            "Epoch [6/10], Step [10/97], Loss: 0.0183\n",
            "Epoch [6/10], Step [11/97], Loss: 0.4913\n",
            "Epoch [6/10], Step [12/97], Loss: 0.2366\n",
            "Epoch [6/10], Step [13/97], Loss: 0.2541\n",
            "Epoch [6/10], Step [14/97], Loss: 0.2141\n",
            "Epoch [6/10], Step [15/97], Loss: 0.2431\n",
            "Epoch [6/10], Step [16/97], Loss: 0.0032\n",
            "Epoch [6/10], Step [17/97], Loss: 0.0001\n",
            "Epoch [6/10], Step [18/97], Loss: 0.0038\n",
            "Epoch [6/10], Step [19/97], Loss: 0.3029\n",
            "Epoch [6/10], Step [20/97], Loss: 0.0085\n",
            "Epoch [6/10], Step [21/97], Loss: 0.0857\n",
            "Epoch [6/10], Step [22/97], Loss: 0.2467\n",
            "Epoch [6/10], Step [23/97], Loss: 0.0106\n",
            "Epoch [6/10], Step [24/97], Loss: 0.0003\n",
            "Epoch [6/10], Step [25/97], Loss: 0.0288\n",
            "Epoch [6/10], Step [26/97], Loss: 0.0253\n",
            "Epoch [6/10], Step [27/97], Loss: 0.0002\n",
            "Epoch [6/10], Step [28/97], Loss: 0.0376\n",
            "Epoch [6/10], Step [29/97], Loss: 0.0058\n",
            "Epoch [6/10], Step [30/97], Loss: 0.0709\n",
            "Epoch [6/10], Step [31/97], Loss: 0.0003\n",
            "Epoch [6/10], Step [32/97], Loss: 0.0863\n",
            "Epoch [6/10], Step [33/97], Loss: 0.0945\n",
            "Epoch [6/10], Step [34/97], Loss: 0.0577\n",
            "Epoch [6/10], Step [35/97], Loss: 0.1242\n",
            "Epoch [6/10], Step [36/97], Loss: 0.0754\n",
            "Epoch [6/10], Step [37/97], Loss: 0.0596\n",
            "Epoch [6/10], Step [38/97], Loss: 0.0538\n",
            "Epoch [6/10], Step [39/97], Loss: 0.0107\n",
            "Epoch [6/10], Step [40/97], Loss: 0.0756\n",
            "Epoch [6/10], Step [41/97], Loss: 0.0053\n",
            "Epoch [6/10], Step [42/97], Loss: 0.0259\n",
            "Epoch [6/10], Step [43/97], Loss: 0.0025\n",
            "Epoch [6/10], Step [44/97], Loss: 0.0367\n",
            "Epoch [6/10], Step [45/97], Loss: 0.0182\n",
            "Epoch [6/10], Step [46/97], Loss: 0.0325\n",
            "Epoch [6/10], Step [47/97], Loss: 0.0177\n",
            "Epoch [6/10], Step [48/97], Loss: 0.0885\n",
            "Epoch [6/10], Step [49/97], Loss: 0.0012\n",
            "Epoch [6/10], Step [50/97], Loss: 0.0003\n",
            "Epoch [6/10], Step [51/97], Loss: 0.1183\n",
            "Epoch [6/10], Step [52/97], Loss: 0.0044\n",
            "Epoch [6/10], Step [53/97], Loss: 0.0383\n",
            "Epoch [6/10], Step [54/97], Loss: 0.0232\n",
            "Epoch [6/10], Step [55/97], Loss: 0.0001\n",
            "Epoch [6/10], Step [56/97], Loss: 0.0006\n",
            "Epoch [6/10], Step [57/97], Loss: 0.1459\n",
            "Epoch [6/10], Step [58/97], Loss: 0.0302\n",
            "Epoch [6/10], Step [59/97], Loss: 0.0010\n",
            "Epoch [6/10], Step [60/97], Loss: 0.2151\n",
            "Epoch [6/10], Step [61/97], Loss: 0.0111\n",
            "Epoch [6/10], Step [62/97], Loss: 0.0026\n",
            "Epoch [6/10], Step [63/97], Loss: 0.0072\n",
            "Epoch [6/10], Step [64/97], Loss: 0.0323\n",
            "Epoch [6/10], Step [65/97], Loss: 0.0495\n",
            "Epoch [6/10], Step [66/97], Loss: 0.0292\n",
            "Epoch [6/10], Step [67/97], Loss: 0.2078\n",
            "Epoch [6/10], Step [68/97], Loss: 0.1073\n",
            "Epoch [6/10], Step [69/97], Loss: 0.1679\n",
            "Epoch [6/10], Step [70/97], Loss: 0.0031\n",
            "Epoch [6/10], Step [71/97], Loss: 0.0613\n",
            "Epoch [6/10], Step [72/97], Loss: 0.0020\n",
            "Epoch [6/10], Step [73/97], Loss: 0.0020\n",
            "Epoch [6/10], Step [74/97], Loss: 0.1692\n",
            "Epoch [6/10], Step [75/97], Loss: 0.0060\n",
            "Epoch [6/10], Step [76/97], Loss: 0.0017\n",
            "Epoch [6/10], Step [77/97], Loss: 0.0803\n",
            "Epoch [6/10], Step [78/97], Loss: 0.0484\n",
            "Epoch [6/10], Step [79/97], Loss: 0.2674\n",
            "Epoch [6/10], Step [80/97], Loss: 0.2151\n",
            "Epoch [6/10], Step [81/97], Loss: 0.0540\n",
            "Epoch [6/10], Step [82/97], Loss: 0.0767\n",
            "Epoch [6/10], Step [83/97], Loss: 0.0920\n",
            "Epoch [6/10], Step [84/97], Loss: 0.0168\n",
            "Epoch [6/10], Step [85/97], Loss: 0.0544\n",
            "Epoch [6/10], Step [86/97], Loss: 0.1214\n",
            "Epoch [6/10], Step [87/97], Loss: 0.0365\n",
            "Epoch [6/10], Step [88/97], Loss: 0.0496\n",
            "Epoch [6/10], Step [89/97], Loss: 0.0402\n",
            "Epoch [6/10], Step [90/97], Loss: 0.1769\n",
            "Epoch [6/10], Step [91/97], Loss: 0.0046\n",
            "Epoch [6/10], Step [92/97], Loss: 0.0015\n",
            "Epoch [6/10], Step [93/97], Loss: 0.2203\n",
            "Epoch [6/10], Step [94/97], Loss: 0.1849\n",
            "Epoch [6/10], Step [95/97], Loss: 0.0009\n",
            "Epoch [6/10], Step [96/97], Loss: 0.0098\n",
            "Epoch [6/10], Step [97/97], Loss: 0.0089\n",
            "Epoch finished\n",
            "Epoch [7/10], Step [1/97], Loss: 0.0534\n",
            "Epoch [7/10], Step [2/97], Loss: 0.0260\n",
            "Epoch [7/10], Step [3/97], Loss: 0.0687\n",
            "Epoch [7/10], Step [4/97], Loss: 0.0114\n",
            "Epoch [7/10], Step [5/97], Loss: 0.0950\n",
            "Epoch [7/10], Step [6/97], Loss: 0.0007\n",
            "Epoch [7/10], Step [7/97], Loss: 0.0344\n",
            "Epoch [7/10], Step [8/97], Loss: 0.0023\n",
            "Epoch [7/10], Step [9/97], Loss: 0.0838\n",
            "Epoch [7/10], Step [10/97], Loss: 0.0118\n",
            "Epoch [7/10], Step [11/97], Loss: 0.3361\n",
            "Epoch [7/10], Step [12/97], Loss: 0.1493\n",
            "Epoch [7/10], Step [13/97], Loss: 0.0124\n",
            "Epoch [7/10], Step [14/97], Loss: 0.0092\n",
            "Epoch [7/10], Step [15/97], Loss: 0.0573\n",
            "Epoch [7/10], Step [16/97], Loss: 0.2026\n",
            "Epoch [7/10], Step [17/97], Loss: 0.2237\n",
            "Epoch [7/10], Step [18/97], Loss: 0.1633\n",
            "Epoch [7/10], Step [19/97], Loss: 0.1056\n",
            "Epoch [7/10], Step [20/97], Loss: 0.0210\n",
            "Epoch [7/10], Step [21/97], Loss: 0.0089\n",
            "Epoch [7/10], Step [22/97], Loss: 0.2453\n",
            "Epoch [7/10], Step [23/97], Loss: 0.0492\n",
            "Epoch [7/10], Step [24/97], Loss: 0.0695\n",
            "Epoch [7/10], Step [25/97], Loss: 0.1010\n",
            "Epoch [7/10], Step [26/97], Loss: 0.0694\n",
            "Epoch [7/10], Step [27/97], Loss: 0.0035\n",
            "Epoch [7/10], Step [28/97], Loss: 0.0089\n",
            "Epoch [7/10], Step [29/97], Loss: 0.0412\n",
            "Epoch [7/10], Step [30/97], Loss: 0.0034\n",
            "Epoch [7/10], Step [31/97], Loss: 0.0002\n",
            "Epoch [7/10], Step [32/97], Loss: 0.0264\n",
            "Epoch [7/10], Step [33/97], Loss: 0.0059\n",
            "Epoch [7/10], Step [34/97], Loss: 0.0180\n",
            "Epoch [7/10], Step [35/97], Loss: 0.2991\n",
            "Epoch [7/10], Step [36/97], Loss: 0.0106\n",
            "Epoch [7/10], Step [37/97], Loss: 0.0061\n",
            "Epoch [7/10], Step [38/97], Loss: 0.0048\n",
            "Epoch [7/10], Step [39/97], Loss: 0.0120\n",
            "Epoch [7/10], Step [40/97], Loss: 0.1531\n",
            "Epoch [7/10], Step [41/97], Loss: 0.0091\n",
            "Epoch [7/10], Step [42/97], Loss: 0.0969\n",
            "Epoch [7/10], Step [43/97], Loss: 0.0036\n",
            "Epoch [7/10], Step [44/97], Loss: 0.0031\n",
            "Epoch [7/10], Step [45/97], Loss: 0.2652\n",
            "Epoch [7/10], Step [46/97], Loss: 0.0419\n",
            "Epoch [7/10], Step [47/97], Loss: 0.1566\n",
            "Epoch [7/10], Step [48/97], Loss: 0.0416\n",
            "Epoch [7/10], Step [49/97], Loss: 0.0941\n",
            "Epoch [7/10], Step [50/97], Loss: 0.0018\n",
            "Epoch [7/10], Step [51/97], Loss: 0.0210\n",
            "Epoch [7/10], Step [52/97], Loss: 0.2626\n",
            "Epoch [7/10], Step [53/97], Loss: 0.0627\n",
            "Epoch [7/10], Step [54/97], Loss: 0.0448\n",
            "Epoch [7/10], Step [55/97], Loss: 0.0009\n",
            "Epoch [7/10], Step [56/97], Loss: 0.0909\n",
            "Epoch [7/10], Step [57/97], Loss: 0.1247\n",
            "Epoch [7/10], Step [58/97], Loss: 0.0035\n",
            "Epoch [7/10], Step [59/97], Loss: 0.0014\n",
            "Epoch [7/10], Step [60/97], Loss: 0.0566\n",
            "Epoch [7/10], Step [61/97], Loss: 0.0242\n",
            "Epoch [7/10], Step [62/97], Loss: 0.0342\n",
            "Epoch [7/10], Step [63/97], Loss: 0.0902\n",
            "Epoch [7/10], Step [64/97], Loss: 0.0030\n",
            "Epoch [7/10], Step [65/97], Loss: 0.1548\n",
            "Epoch [7/10], Step [66/97], Loss: 0.2178\n",
            "Epoch [7/10], Step [67/97], Loss: 0.1320\n",
            "Epoch [7/10], Step [68/97], Loss: 0.0040\n",
            "Epoch [7/10], Step [69/97], Loss: 0.0082\n",
            "Epoch [7/10], Step [70/97], Loss: 0.0159\n",
            "Epoch [7/10], Step [71/97], Loss: 0.0024\n",
            "Epoch [7/10], Step [72/97], Loss: 0.0390\n",
            "Epoch [7/10], Step [73/97], Loss: 0.2475\n",
            "Epoch [7/10], Step [74/97], Loss: 0.0870\n",
            "Epoch [7/10], Step [75/97], Loss: 0.0775\n",
            "Epoch [7/10], Step [76/97], Loss: 0.0199\n",
            "Epoch [7/10], Step [77/97], Loss: 0.0040\n",
            "Epoch [7/10], Step [78/97], Loss: 0.1383\n",
            "Epoch [7/10], Step [79/97], Loss: 0.0071\n",
            "Epoch [7/10], Step [80/97], Loss: 0.0438\n",
            "Epoch [7/10], Step [81/97], Loss: 0.0017\n",
            "Epoch [7/10], Step [82/97], Loss: 0.0233\n",
            "Epoch [7/10], Step [83/97], Loss: 0.0795\n",
            "Epoch [7/10], Step [84/97], Loss: 0.0821\n",
            "Epoch [7/10], Step [85/97], Loss: 0.0026\n",
            "Epoch [7/10], Step [86/97], Loss: 0.0402\n",
            "Epoch [7/10], Step [87/97], Loss: 0.1072\n",
            "Epoch [7/10], Step [88/97], Loss: 0.0002\n",
            "Epoch [7/10], Step [89/97], Loss: 0.0408\n",
            "Epoch [7/10], Step [90/97], Loss: 0.0197\n",
            "Epoch [7/10], Step [91/97], Loss: 0.0286\n",
            "Epoch [7/10], Step [92/97], Loss: 0.0101\n",
            "Epoch [7/10], Step [93/97], Loss: 0.0933\n",
            "Epoch [7/10], Step [94/97], Loss: 0.0003\n",
            "Epoch [7/10], Step [95/97], Loss: 0.0709\n",
            "Epoch [7/10], Step [96/97], Loss: 0.0132\n",
            "Epoch [7/10], Step [97/97], Loss: 0.0690\n",
            "Epoch finished\n",
            "Epoch [8/10], Step [1/97], Loss: 0.0039\n",
            "Epoch [8/10], Step [2/97], Loss: 0.1165\n",
            "Epoch [8/10], Step [3/97], Loss: 0.1345\n",
            "Epoch [8/10], Step [4/97], Loss: 0.0137\n",
            "Epoch [8/10], Step [5/97], Loss: 0.0001\n",
            "Epoch [8/10], Step [6/97], Loss: 0.0218\n",
            "Epoch [8/10], Step [7/97], Loss: 0.0919\n",
            "Epoch [8/10], Step [8/97], Loss: 0.1158\n",
            "Epoch [8/10], Step [9/97], Loss: 0.0534\n",
            "Epoch [8/10], Step [10/97], Loss: 0.0885\n",
            "Epoch [8/10], Step [11/97], Loss: 0.1763\n",
            "Epoch [8/10], Step [12/97], Loss: 0.0043\n",
            "Epoch [8/10], Step [13/97], Loss: 0.0067\n",
            "Epoch [8/10], Step [14/97], Loss: 0.0052\n",
            "Epoch [8/10], Step [15/97], Loss: 0.3042\n",
            "Epoch [8/10], Step [16/97], Loss: 0.1205\n",
            "Epoch [8/10], Step [17/97], Loss: 0.2506\n",
            "Epoch [8/10], Step [18/97], Loss: 0.1050\n",
            "Epoch [8/10], Step [19/97], Loss: 0.0660\n",
            "Epoch [8/10], Step [20/97], Loss: 0.0147\n",
            "Epoch [8/10], Step [21/97], Loss: 0.0698\n",
            "Epoch [8/10], Step [22/97], Loss: 0.0418\n",
            "Epoch [8/10], Step [23/97], Loss: 0.0772\n",
            "Epoch [8/10], Step [24/97], Loss: 0.2778\n",
            "Epoch [8/10], Step [25/97], Loss: 0.2664\n",
            "Epoch [8/10], Step [26/97], Loss: 0.0004\n",
            "Epoch [8/10], Step [27/97], Loss: 0.0189\n",
            "Epoch [8/10], Step [28/97], Loss: 0.2717\n",
            "Epoch [8/10], Step [29/97], Loss: 0.0588\n",
            "Epoch [8/10], Step [30/97], Loss: 0.4966\n",
            "Epoch [8/10], Step [31/97], Loss: 0.1780\n",
            "Epoch [8/10], Step [32/97], Loss: 0.0639\n",
            "Epoch [8/10], Step [33/97], Loss: 0.0279\n",
            "Epoch [8/10], Step [34/97], Loss: 0.0065\n",
            "Epoch [8/10], Step [35/97], Loss: 0.1209\n",
            "Epoch [8/10], Step [36/97], Loss: 0.1541\n",
            "Epoch [8/10], Step [37/97], Loss: 0.0002\n",
            "Epoch [8/10], Step [38/97], Loss: 0.0398\n",
            "Epoch [8/10], Step [39/97], Loss: 0.0180\n",
            "Epoch [8/10], Step [40/97], Loss: 0.2051\n",
            "Epoch [8/10], Step [41/97], Loss: 0.1472\n",
            "Epoch [8/10], Step [42/97], Loss: 0.0054\n",
            "Epoch [8/10], Step [43/97], Loss: 0.0056\n",
            "Epoch [8/10], Step [44/97], Loss: 0.0001\n",
            "Epoch [8/10], Step [45/97], Loss: 0.1170\n",
            "Epoch [8/10], Step [46/97], Loss: 0.0757\n",
            "Epoch [8/10], Step [47/97], Loss: 0.0152\n",
            "Epoch [8/10], Step [48/97], Loss: 0.0142\n",
            "Epoch [8/10], Step [49/97], Loss: 0.0378\n",
            "Epoch [8/10], Step [50/97], Loss: 0.0285\n",
            "Epoch [8/10], Step [51/97], Loss: 0.0150\n",
            "Epoch [8/10], Step [52/97], Loss: 0.0984\n",
            "Epoch [8/10], Step [53/97], Loss: 0.2253\n",
            "Epoch [8/10], Step [54/97], Loss: 0.0634\n",
            "Epoch [8/10], Step [55/97], Loss: 0.0767\n",
            "Epoch [8/10], Step [56/97], Loss: 0.0217\n",
            "Epoch [8/10], Step [57/97], Loss: 0.0527\n",
            "Epoch [8/10], Step [58/97], Loss: 0.0034\n",
            "Epoch [8/10], Step [59/97], Loss: 0.0654\n",
            "Epoch [8/10], Step [60/97], Loss: 0.2357\n",
            "Epoch [8/10], Step [61/97], Loss: 0.0127\n",
            "Epoch [8/10], Step [62/97], Loss: 0.0172\n",
            "Epoch [8/10], Step [63/97], Loss: 0.0027\n",
            "Epoch [8/10], Step [64/97], Loss: 0.1385\n",
            "Epoch [8/10], Step [65/97], Loss: 0.0308\n",
            "Epoch [8/10], Step [66/97], Loss: 0.0277\n",
            "Epoch [8/10], Step [67/97], Loss: 0.1101\n",
            "Epoch [8/10], Step [68/97], Loss: 0.0186\n",
            "Epoch [8/10], Step [69/97], Loss: 0.0541\n",
            "Epoch [8/10], Step [70/97], Loss: 0.0634\n",
            "Epoch [8/10], Step [71/97], Loss: 0.1160\n",
            "Epoch [8/10], Step [72/97], Loss: 0.0846\n",
            "Epoch [8/10], Step [73/97], Loss: 0.0016\n",
            "Epoch [8/10], Step [74/97], Loss: 0.1117\n",
            "Epoch [8/10], Step [75/97], Loss: 0.0006\n",
            "Epoch [8/10], Step [76/97], Loss: 0.0492\n",
            "Epoch [8/10], Step [77/97], Loss: 0.0128\n",
            "Epoch [8/10], Step [78/97], Loss: 0.0790\n",
            "Epoch [8/10], Step [79/97], Loss: 0.0756\n",
            "Epoch [8/10], Step [80/97], Loss: 0.0028\n",
            "Epoch [8/10], Step [81/97], Loss: 0.0002\n",
            "Epoch [8/10], Step [82/97], Loss: 0.0240\n",
            "Epoch [8/10], Step [83/97], Loss: 0.1713\n",
            "Epoch [8/10], Step [84/97], Loss: 0.3882\n",
            "Epoch [8/10], Step [85/97], Loss: 0.1360\n",
            "Epoch [8/10], Step [86/97], Loss: 0.0294\n",
            "Epoch [8/10], Step [87/97], Loss: 0.0048\n",
            "Epoch [8/10], Step [88/97], Loss: 0.2998\n",
            "Epoch [8/10], Step [89/97], Loss: 0.4442\n",
            "Epoch [8/10], Step [90/97], Loss: 0.0782\n",
            "Epoch [8/10], Step [91/97], Loss: 0.0006\n",
            "Epoch [8/10], Step [92/97], Loss: 0.0362\n",
            "Epoch [8/10], Step [93/97], Loss: 0.1342\n",
            "Epoch [8/10], Step [94/97], Loss: 0.3263\n",
            "Epoch [8/10], Step [95/97], Loss: 0.0024\n",
            "Epoch [8/10], Step [96/97], Loss: 0.0035\n",
            "Epoch [8/10], Step [97/97], Loss: 0.0759\n",
            "Epoch finished\n",
            "Epoch [9/10], Step [1/97], Loss: 0.0328\n",
            "Epoch [9/10], Step [2/97], Loss: 0.1965\n",
            "Epoch [9/10], Step [3/97], Loss: 0.1032\n",
            "Epoch [9/10], Step [4/97], Loss: 0.0761\n",
            "Epoch [9/10], Step [5/97], Loss: 0.0046\n",
            "Epoch [9/10], Step [6/97], Loss: 0.0236\n",
            "Epoch [9/10], Step [7/97], Loss: 0.4093\n",
            "Epoch [9/10], Step [8/97], Loss: 0.0410\n",
            "Epoch [9/10], Step [9/97], Loss: 0.2488\n",
            "Epoch [9/10], Step [10/97], Loss: 0.0201\n",
            "Epoch [9/10], Step [11/97], Loss: 0.0137\n",
            "Epoch [9/10], Step [12/97], Loss: 0.2201\n",
            "Epoch [9/10], Step [13/97], Loss: 0.0197\n",
            "Epoch [9/10], Step [14/97], Loss: 0.1850\n",
            "Epoch [9/10], Step [15/97], Loss: 0.5247\n",
            "Epoch [9/10], Step [16/97], Loss: 0.0749\n",
            "Epoch [9/10], Step [17/97], Loss: 0.1174\n",
            "Epoch [9/10], Step [18/97], Loss: 0.0014\n",
            "Epoch [9/10], Step [19/97], Loss: 0.1254\n",
            "Epoch [9/10], Step [20/97], Loss: 0.0162\n",
            "Epoch [9/10], Step [21/97], Loss: 0.0432\n",
            "Epoch [9/10], Step [22/97], Loss: 0.0765\n",
            "Epoch [9/10], Step [23/97], Loss: 0.0333\n",
            "Epoch [9/10], Step [24/97], Loss: 0.0002\n",
            "Epoch [9/10], Step [25/97], Loss: 0.0678\n",
            "Epoch [9/10], Step [26/97], Loss: 0.0734\n",
            "Epoch [9/10], Step [27/97], Loss: 0.0000\n",
            "Epoch [9/10], Step [28/97], Loss: 0.0013\n",
            "Epoch [9/10], Step [29/97], Loss: 0.0221\n",
            "Epoch [9/10], Step [30/97], Loss: 0.2445\n",
            "Epoch [9/10], Step [31/97], Loss: 0.0693\n",
            "Epoch [9/10], Step [32/97], Loss: 0.0535\n",
            "Epoch [9/10], Step [33/97], Loss: 0.0087\n",
            "Epoch [9/10], Step [34/97], Loss: 0.1703\n",
            "Epoch [9/10], Step [35/97], Loss: 0.2629\n",
            "Epoch [9/10], Step [36/97], Loss: 0.0317\n",
            "Epoch [9/10], Step [37/97], Loss: 0.0009\n",
            "Epoch [9/10], Step [38/97], Loss: 0.0014\n",
            "Epoch [9/10], Step [39/97], Loss: 0.0019\n",
            "Epoch [9/10], Step [40/97], Loss: 0.1456\n",
            "Epoch [9/10], Step [41/97], Loss: 0.0545\n",
            "Epoch [9/10], Step [42/97], Loss: 0.2257\n",
            "Epoch [9/10], Step [43/97], Loss: 0.0318\n",
            "Epoch [9/10], Step [44/97], Loss: 0.0962\n",
            "Epoch [9/10], Step [45/97], Loss: 0.0118\n",
            "Epoch [9/10], Step [46/97], Loss: 0.0323\n",
            "Epoch [9/10], Step [47/97], Loss: 0.0044\n",
            "Epoch [9/10], Step [48/97], Loss: 0.5069\n",
            "Epoch [9/10], Step [49/97], Loss: 0.0552\n",
            "Epoch [9/10], Step [50/97], Loss: 0.0915\n",
            "Epoch [9/10], Step [51/97], Loss: 0.1296\n",
            "Epoch [9/10], Step [52/97], Loss: 0.0446\n",
            "Epoch [9/10], Step [53/97], Loss: 0.6825\n",
            "Epoch [9/10], Step [54/97], Loss: 0.0545\n",
            "Epoch [9/10], Step [55/97], Loss: 0.1132\n",
            "Epoch [9/10], Step [56/97], Loss: 0.0144\n",
            "Epoch [9/10], Step [57/97], Loss: 0.0026\n",
            "Epoch [9/10], Step [58/97], Loss: 0.2036\n",
            "Epoch [9/10], Step [59/97], Loss: 0.0419\n",
            "Epoch [9/10], Step [60/97], Loss: 0.0015\n",
            "Epoch [9/10], Step [61/97], Loss: 0.1032\n",
            "Epoch [9/10], Step [62/97], Loss: 0.0024\n",
            "Epoch [9/10], Step [63/97], Loss: 0.1494\n",
            "Epoch [9/10], Step [64/97], Loss: 0.1879\n",
            "Epoch [9/10], Step [65/97], Loss: 0.1226\n",
            "Epoch [9/10], Step [66/97], Loss: 0.0200\n",
            "Epoch [9/10], Step [67/97], Loss: 0.0456\n",
            "Epoch [9/10], Step [68/97], Loss: 0.0807\n",
            "Epoch [9/10], Step [69/97], Loss: 0.0010\n",
            "Epoch [9/10], Step [70/97], Loss: 0.0437\n",
            "Epoch [9/10], Step [71/97], Loss: 0.0015\n",
            "Epoch [9/10], Step [72/97], Loss: 0.0002\n",
            "Epoch [9/10], Step [73/97], Loss: 0.0784\n",
            "Epoch [9/10], Step [74/97], Loss: 0.0000\n",
            "Epoch [9/10], Step [75/97], Loss: 0.0020\n",
            "Epoch [9/10], Step [76/97], Loss: 0.0317\n",
            "Epoch [9/10], Step [77/97], Loss: 0.0526\n",
            "Epoch [9/10], Step [78/97], Loss: 0.1143\n",
            "Epoch [9/10], Step [79/97], Loss: 0.0417\n",
            "Epoch [9/10], Step [80/97], Loss: 0.1628\n",
            "Epoch [9/10], Step [81/97], Loss: 0.0740\n",
            "Epoch [9/10], Step [82/97], Loss: 0.0005\n",
            "Epoch [9/10], Step [83/97], Loss: 0.0822\n",
            "Epoch [9/10], Step [84/97], Loss: 0.1418\n",
            "Epoch [9/10], Step [85/97], Loss: 0.0000\n",
            "Epoch [9/10], Step [86/97], Loss: 0.1772\n",
            "Epoch [9/10], Step [87/97], Loss: 0.0174\n",
            "Epoch [9/10], Step [88/97], Loss: 0.1115\n",
            "Epoch [9/10], Step [89/97], Loss: 0.1985\n",
            "Epoch [9/10], Step [90/97], Loss: 0.0851\n",
            "Epoch [9/10], Step [91/97], Loss: 0.0490\n",
            "Epoch [9/10], Step [92/97], Loss: 0.0157\n",
            "Epoch [9/10], Step [93/97], Loss: 0.1733\n",
            "Epoch [9/10], Step [94/97], Loss: 0.2858\n",
            "Epoch [9/10], Step [95/97], Loss: 0.0749\n",
            "Epoch [9/10], Step [96/97], Loss: 0.0018\n",
            "Epoch [9/10], Step [97/97], Loss: 0.0064\n",
            "Epoch finished\n",
            "Epoch [10/10], Step [1/97], Loss: 0.0535\n",
            "Epoch [10/10], Step [2/97], Loss: 0.0768\n",
            "Epoch [10/10], Step [3/97], Loss: 0.0260\n",
            "Epoch [10/10], Step [4/97], Loss: 0.0032\n",
            "Epoch [10/10], Step [5/97], Loss: 0.0001\n",
            "Epoch [10/10], Step [6/97], Loss: 0.0207\n",
            "Epoch [10/10], Step [7/97], Loss: 0.4093\n",
            "Epoch [10/10], Step [8/97], Loss: 0.0041\n",
            "Epoch [10/10], Step [9/97], Loss: 0.1619\n",
            "Epoch [10/10], Step [10/97], Loss: 0.0172\n",
            "Epoch [10/10], Step [11/97], Loss: 0.3145\n",
            "Epoch [10/10], Step [12/97], Loss: 0.0784\n",
            "Epoch [10/10], Step [13/97], Loss: 0.0539\n",
            "Epoch [10/10], Step [14/97], Loss: 0.0034\n",
            "Epoch [10/10], Step [15/97], Loss: 0.2911\n",
            "Epoch [10/10], Step [16/97], Loss: 0.0555\n",
            "Epoch [10/10], Step [17/97], Loss: 0.0863\n",
            "Epoch [10/10], Step [18/97], Loss: 0.0102\n",
            "Epoch [10/10], Step [19/97], Loss: 0.0921\n",
            "Epoch [10/10], Step [20/97], Loss: 0.0026\n",
            "Epoch [10/10], Step [21/97], Loss: 0.3344\n",
            "Epoch [10/10], Step [22/97], Loss: 0.4230\n",
            "Epoch [10/10], Step [23/97], Loss: 0.0366\n",
            "Epoch [10/10], Step [24/97], Loss: 0.0039\n",
            "Epoch [10/10], Step [25/97], Loss: 0.1482\n",
            "Epoch [10/10], Step [26/97], Loss: 0.0174\n",
            "Epoch [10/10], Step [27/97], Loss: 0.0000\n",
            "Epoch [10/10], Step [28/97], Loss: 0.1353\n",
            "Epoch [10/10], Step [29/97], Loss: 0.1690\n",
            "Epoch [10/10], Step [30/97], Loss: 0.1579\n",
            "Epoch [10/10], Step [31/97], Loss: 0.0002\n",
            "Epoch [10/10], Step [32/97], Loss: 0.0021\n",
            "Epoch [10/10], Step [33/97], Loss: 0.0239\n",
            "Epoch [10/10], Step [34/97], Loss: 0.1179\n",
            "Epoch [10/10], Step [35/97], Loss: 0.2593\n",
            "Epoch [10/10], Step [36/97], Loss: 0.0463\n",
            "Epoch [10/10], Step [37/97], Loss: 0.0224\n",
            "Epoch [10/10], Step [38/97], Loss: 0.0135\n",
            "Epoch [10/10], Step [39/97], Loss: 0.0428\n",
            "Epoch [10/10], Step [40/97], Loss: 0.0156\n",
            "Epoch [10/10], Step [41/97], Loss: 0.0001\n",
            "Epoch [10/10], Step [42/97], Loss: 0.1767\n",
            "Epoch [10/10], Step [43/97], Loss: 0.0870\n",
            "Epoch [10/10], Step [44/97], Loss: 0.0002\n",
            "Epoch [10/10], Step [45/97], Loss: 0.0002\n",
            "Epoch [10/10], Step [46/97], Loss: 0.1920\n",
            "Epoch [10/10], Step [47/97], Loss: 0.0003\n",
            "Epoch [10/10], Step [48/97], Loss: 0.0035\n",
            "Epoch [10/10], Step [49/97], Loss: 0.0591\n",
            "Epoch [10/10], Step [50/97], Loss: 0.0072\n",
            "Epoch [10/10], Step [51/97], Loss: 0.1152\n",
            "Epoch [10/10], Step [52/97], Loss: 0.0642\n",
            "Epoch [10/10], Step [53/97], Loss: 0.0201\n",
            "Epoch [10/10], Step [54/97], Loss: 0.0158\n",
            "Epoch [10/10], Step [55/97], Loss: 0.0150\n",
            "Epoch [10/10], Step [56/97], Loss: 0.0075\n",
            "Epoch [10/10], Step [57/97], Loss: 0.1975\n",
            "Epoch [10/10], Step [58/97], Loss: 0.0057\n",
            "Epoch [10/10], Step [59/97], Loss: 0.0047\n",
            "Epoch [10/10], Step [60/97], Loss: 0.2235\n",
            "Epoch [10/10], Step [61/97], Loss: 0.1271\n",
            "Epoch [10/10], Step [62/97], Loss: 0.0002\n",
            "Epoch [10/10], Step [63/97], Loss: 0.0365\n",
            "Epoch [10/10], Step [64/97], Loss: 0.1476\n",
            "Epoch [10/10], Step [65/97], Loss: 0.0313\n",
            "Epoch [10/10], Step [66/97], Loss: 0.0013\n",
            "Epoch [10/10], Step [67/97], Loss: 0.2407\n",
            "Epoch [10/10], Step [68/97], Loss: 0.1398\n",
            "Epoch [10/10], Step [69/97], Loss: 0.1578\n",
            "Epoch [10/10], Step [70/97], Loss: 0.0016\n",
            "Epoch [10/10], Step [71/97], Loss: 0.0003\n",
            "Epoch [10/10], Step [72/97], Loss: 0.0038\n",
            "Epoch [10/10], Step [73/97], Loss: 0.1001\n",
            "Epoch [10/10], Step [74/97], Loss: 0.0010\n",
            "Epoch [10/10], Step [75/97], Loss: 0.0025\n",
            "Epoch [10/10], Step [76/97], Loss: 0.0440\n",
            "Epoch [10/10], Step [77/97], Loss: 0.0045\n",
            "Epoch [10/10], Step [78/97], Loss: 0.2122\n",
            "Epoch [10/10], Step [79/97], Loss: 0.0065\n",
            "Epoch [10/10], Step [80/97], Loss: 0.0138\n",
            "Epoch [10/10], Step [81/97], Loss: 0.0002\n",
            "Epoch [10/10], Step [82/97], Loss: 0.0217\n",
            "Epoch [10/10], Step [83/97], Loss: 0.0511\n",
            "Epoch [10/10], Step [84/97], Loss: 0.0631\n",
            "Epoch [10/10], Step [85/97], Loss: 0.1286\n",
            "Epoch [10/10], Step [86/97], Loss: 0.1044\n",
            "Epoch [10/10], Step [87/97], Loss: 0.0318\n",
            "Epoch [10/10], Step [88/97], Loss: 0.0153\n",
            "Epoch [10/10], Step [89/97], Loss: 0.1032\n",
            "Epoch [10/10], Step [90/97], Loss: 0.1374\n",
            "Epoch [10/10], Step [91/97], Loss: 0.0078\n",
            "Epoch [10/10], Step [92/97], Loss: 0.1582\n",
            "Epoch [10/10], Step [93/97], Loss: 0.0639\n",
            "Epoch [10/10], Step [94/97], Loss: 0.4026\n",
            "Epoch [10/10], Step [95/97], Loss: 0.0006\n",
            "Epoch [10/10], Step [96/97], Loss: 0.1926\n",
            "Epoch [10/10], Step [97/97], Loss: 0.0063\n",
            "Epoch finished\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = alex(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MXig2jFuBZR",
        "outputId": "c55e081f-256a-4fed-97c5-0bf982c69db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 98 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "nBeOHsjauEwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = models.vgg16(pretrained=True)"
      ],
      "metadata": {
        "id": "Zi7SLSZk9f_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freez the model\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad=False"
      ],
      "metadata": {
        "id": "vtm76JNVuRtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "n_inputs = vgg.classifier[6].in_features\n",
        "vgg.classifier[6]=nn.Sequential(  nn.Linear(n_inputs, 256),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(p=0.4),\n",
        "                                nn.Linear(256,len(classes)),\n",
        "                                nn.LogSoftmax(dim=1))\n",
        "print(vgg.classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooy5joOpuVTq",
        "outputId": "e9dce8db-3184-4b58-e265-4c559343ded6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Sequential(\n",
            "    (0): Linear(in_features=4096, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.4, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=8, bias=True)\n",
            "    (4): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import lr_scheduler\n",
        "# Negative Log Likelihood Loss\n",
        "criterion=nn.NLLLoss()\n",
        "optimizer=torch.optim.Adam(vgg.classifier.parameters(),lr=0.0001)\n",
        "# learning rate scheduler (adjusts the learning rate during training.)\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "d20-LWSLveaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = vgg.to(device)"
      ],
      "metadata": {
        "id": "H2un4iEXLQKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBZi-gftLaE_",
        "outputId": "4c8c61ef-42d1-4957-860a-330cc5c2b436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Sequential(\n",
              "      (0): Linear(in_features=4096, out_features=256, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Dropout(p=0.4, inplace=False)\n",
              "      (3): Linear(in_features=256, out_features=8, bias=True)\n",
              "      (4): LogSoftmax(dim=1)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "total_step = len(train_loader)\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = vgg(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # exp_lr_scheduler.step()\n",
        "\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    print('Epoch finished')\n",
        "\n",
        "print('Training finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ShOMSeVvhGv",
        "outputId": "75857dc1-f220-4147-e579-1bcf140f69ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [1/38], Loss: 2.1111\n",
            "Epoch [1/10], Step [2/38], Loss: 2.0618\n",
            "Epoch [1/10], Step [3/38], Loss: 1.9665\n",
            "Epoch [1/10], Step [4/38], Loss: 1.8460\n",
            "Epoch [1/10], Step [5/38], Loss: 1.8195\n",
            "Epoch [1/10], Step [6/38], Loss: 1.7349\n",
            "Epoch [1/10], Step [7/38], Loss: 1.6712\n",
            "Epoch [1/10], Step [8/38], Loss: 1.6398\n",
            "Epoch [1/10], Step [9/38], Loss: 1.5295\n",
            "Epoch [1/10], Step [10/38], Loss: 1.4711\n",
            "Epoch [1/10], Step [11/38], Loss: 1.3532\n",
            "Epoch [1/10], Step [12/38], Loss: 1.3090\n",
            "Epoch [1/10], Step [13/38], Loss: 1.2115\n",
            "Epoch [1/10], Step [14/38], Loss: 1.2002\n",
            "Epoch [1/10], Step [15/38], Loss: 1.0637\n",
            "Epoch [1/10], Step [16/38], Loss: 1.0365\n",
            "Epoch [1/10], Step [17/38], Loss: 1.0035\n",
            "Epoch [1/10], Step [18/38], Loss: 0.9513\n",
            "Epoch [1/10], Step [19/38], Loss: 0.8760\n",
            "Epoch [1/10], Step [20/38], Loss: 0.7924\n",
            "Epoch [1/10], Step [21/38], Loss: 0.7275\n",
            "Epoch [1/10], Step [22/38], Loss: 0.6330\n",
            "Epoch [1/10], Step [23/38], Loss: 0.6480\n",
            "Epoch [1/10], Step [24/38], Loss: 0.6429\n",
            "Epoch [1/10], Step [25/38], Loss: 0.5531\n",
            "Epoch [1/10], Step [26/38], Loss: 0.5835\n",
            "Epoch [1/10], Step [27/38], Loss: 0.4829\n",
            "Epoch [1/10], Step [28/38], Loss: 0.4667\n",
            "Epoch [1/10], Step [29/38], Loss: 0.4400\n",
            "Epoch [1/10], Step [30/38], Loss: 0.3769\n",
            "Epoch [1/10], Step [31/38], Loss: 0.4102\n",
            "Epoch [1/10], Step [32/38], Loss: 0.3960\n",
            "Epoch [1/10], Step [33/38], Loss: 0.3471\n",
            "Epoch [1/10], Step [34/38], Loss: 0.3051\n",
            "Epoch [1/10], Step [35/38], Loss: 0.3382\n",
            "Epoch [1/10], Step [36/38], Loss: 0.2914\n",
            "Epoch [1/10], Step [37/38], Loss: 0.3251\n",
            "Epoch [1/10], Step [38/38], Loss: 0.2243\n",
            "Epoch finished\n",
            "Epoch [2/10], Step [1/38], Loss: 0.2600\n",
            "Epoch [2/10], Step [2/38], Loss: 0.2699\n",
            "Epoch [2/10], Step [3/38], Loss: 0.2301\n",
            "Epoch [2/10], Step [4/38], Loss: 0.1835\n",
            "Epoch [2/10], Step [5/38], Loss: 0.2544\n",
            "Epoch [2/10], Step [6/38], Loss: 0.1813\n",
            "Epoch [2/10], Step [7/38], Loss: 0.2099\n",
            "Epoch [2/10], Step [8/38], Loss: 0.2396\n",
            "Epoch [2/10], Step [9/38], Loss: 0.1945\n",
            "Epoch [2/10], Step [10/38], Loss: 0.1524\n",
            "Epoch [2/10], Step [11/38], Loss: 0.1872\n",
            "Epoch [2/10], Step [12/38], Loss: 0.1426\n",
            "Epoch [2/10], Step [13/38], Loss: 0.1745\n",
            "Epoch [2/10], Step [14/38], Loss: 0.1579\n",
            "Epoch [2/10], Step [15/38], Loss: 0.1266\n",
            "Epoch [2/10], Step [16/38], Loss: 0.1525\n",
            "Epoch [2/10], Step [17/38], Loss: 0.1784\n",
            "Epoch [2/10], Step [18/38], Loss: 0.1286\n",
            "Epoch [2/10], Step [19/38], Loss: 0.1627\n",
            "Epoch [2/10], Step [20/38], Loss: 0.1619\n",
            "Epoch [2/10], Step [21/38], Loss: 0.1436\n",
            "Epoch [2/10], Step [22/38], Loss: 0.1035\n",
            "Epoch [2/10], Step [23/38], Loss: 0.1422\n",
            "Epoch [2/10], Step [24/38], Loss: 0.1581\n",
            "Epoch [2/10], Step [25/38], Loss: 0.1401\n",
            "Epoch [2/10], Step [26/38], Loss: 0.1439\n",
            "Epoch [2/10], Step [27/38], Loss: 0.1307\n",
            "Epoch [2/10], Step [28/38], Loss: 0.1371\n",
            "Epoch [2/10], Step [29/38], Loss: 0.1102\n",
            "Epoch [2/10], Step [30/38], Loss: 0.0763\n",
            "Epoch [2/10], Step [31/38], Loss: 0.1497\n",
            "Epoch [2/10], Step [32/38], Loss: 0.1409\n",
            "Epoch [2/10], Step [33/38], Loss: 0.1074\n",
            "Epoch [2/10], Step [34/38], Loss: 0.1156\n",
            "Epoch [2/10], Step [35/38], Loss: 0.1132\n",
            "Epoch [2/10], Step [36/38], Loss: 0.1393\n",
            "Epoch [2/10], Step [37/38], Loss: 0.1489\n",
            "Epoch [2/10], Step [38/38], Loss: 0.0680\n",
            "Epoch finished\n",
            "Epoch [3/10], Step [1/38], Loss: 0.0803\n",
            "Epoch [3/10], Step [2/38], Loss: 0.1026\n",
            "Epoch [3/10], Step [3/38], Loss: 0.1452\n",
            "Epoch [3/10], Step [4/38], Loss: 0.0608\n",
            "Epoch [3/10], Step [5/38], Loss: 0.1327\n",
            "Epoch [3/10], Step [6/38], Loss: 0.0596\n",
            "Epoch [3/10], Step [7/38], Loss: 0.1098\n",
            "Epoch [3/10], Step [8/38], Loss: 0.1072\n",
            "Epoch [3/10], Step [9/38], Loss: 0.0721\n",
            "Epoch [3/10], Step [10/38], Loss: 0.0881\n",
            "Epoch [3/10], Step [11/38], Loss: 0.0833\n",
            "Epoch [3/10], Step [12/38], Loss: 0.0489\n",
            "Epoch [3/10], Step [13/38], Loss: 0.0942\n",
            "Epoch [3/10], Step [14/38], Loss: 0.0752\n",
            "Epoch [3/10], Step [15/38], Loss: 0.0566\n",
            "Epoch [3/10], Step [16/38], Loss: 0.0814\n",
            "Epoch [3/10], Step [17/38], Loss: 0.1366\n",
            "Epoch [3/10], Step [18/38], Loss: 0.0955\n",
            "Epoch [3/10], Step [19/38], Loss: 0.0711\n",
            "Epoch [3/10], Step [20/38], Loss: 0.1240\n",
            "Epoch [3/10], Step [21/38], Loss: 0.0871\n",
            "Epoch [3/10], Step [22/38], Loss: 0.0460\n",
            "Epoch [3/10], Step [23/38], Loss: 0.0714\n",
            "Epoch [3/10], Step [24/38], Loss: 0.0998\n",
            "Epoch [3/10], Step [25/38], Loss: 0.0818\n",
            "Epoch [3/10], Step [26/38], Loss: 0.1081\n",
            "Epoch [3/10], Step [27/38], Loss: 0.0842\n",
            "Epoch [3/10], Step [28/38], Loss: 0.0946\n",
            "Epoch [3/10], Step [29/38], Loss: 0.1055\n",
            "Epoch [3/10], Step [30/38], Loss: 0.1011\n",
            "Epoch [3/10], Step [31/38], Loss: 0.1427\n",
            "Epoch [3/10], Step [32/38], Loss: 0.0779\n",
            "Epoch [3/10], Step [33/38], Loss: 0.0618\n",
            "Epoch [3/10], Step [34/38], Loss: 0.0766\n",
            "Epoch [3/10], Step [35/38], Loss: 0.0839\n",
            "Epoch [3/10], Step [36/38], Loss: 0.0810\n",
            "Epoch [3/10], Step [37/38], Loss: 0.0983\n",
            "Epoch [3/10], Step [38/38], Loss: 0.0701\n",
            "Epoch finished\n",
            "Epoch [4/10], Step [1/38], Loss: 0.0915\n",
            "Epoch [4/10], Step [2/38], Loss: 0.0654\n",
            "Epoch [4/10], Step [3/38], Loss: 0.1057\n",
            "Epoch [4/10], Step [4/38], Loss: 0.0561\n",
            "Epoch [4/10], Step [5/38], Loss: 0.0945\n",
            "Epoch [4/10], Step [6/38], Loss: 0.0513\n",
            "Epoch [4/10], Step [7/38], Loss: 0.0896\n",
            "Epoch [4/10], Step [8/38], Loss: 0.0835\n",
            "Epoch [4/10], Step [9/38], Loss: 0.0746\n",
            "Epoch [4/10], Step [10/38], Loss: 0.0504\n",
            "Epoch [4/10], Step [11/38], Loss: 0.0489\n",
            "Epoch [4/10], Step [12/38], Loss: 0.0450\n",
            "Epoch [4/10], Step [13/38], Loss: 0.0569\n",
            "Epoch [4/10], Step [14/38], Loss: 0.0477\n",
            "Epoch [4/10], Step [15/38], Loss: 0.0469\n",
            "Epoch [4/10], Step [16/38], Loss: 0.0631\n",
            "Epoch [4/10], Step [17/38], Loss: 0.0792\n",
            "Epoch [4/10], Step [18/38], Loss: 0.1105\n",
            "Epoch [4/10], Step [19/38], Loss: 0.0628\n",
            "Epoch [4/10], Step [20/38], Loss: 0.0793\n",
            "Epoch [4/10], Step [21/38], Loss: 0.0432\n",
            "Epoch [4/10], Step [22/38], Loss: 0.0398\n",
            "Epoch [4/10], Step [23/38], Loss: 0.0586\n",
            "Epoch [4/10], Step [24/38], Loss: 0.1076\n",
            "Epoch [4/10], Step [25/38], Loss: 0.0540\n",
            "Epoch [4/10], Step [26/38], Loss: 0.0773\n",
            "Epoch [4/10], Step [27/38], Loss: 0.0781\n",
            "Epoch [4/10], Step [28/38], Loss: 0.0581\n",
            "Epoch [4/10], Step [29/38], Loss: 0.0516\n",
            "Epoch [4/10], Step [30/38], Loss: 0.0366\n",
            "Epoch [4/10], Step [31/38], Loss: 0.0886\n",
            "Epoch [4/10], Step [32/38], Loss: 0.0565\n",
            "Epoch [4/10], Step [33/38], Loss: 0.0618\n",
            "Epoch [4/10], Step [34/38], Loss: 0.0583\n",
            "Epoch [4/10], Step [35/38], Loss: 0.0706\n",
            "Epoch [4/10], Step [36/38], Loss: 0.0797\n",
            "Epoch [4/10], Step [37/38], Loss: 0.0920\n",
            "Epoch [4/10], Step [38/38], Loss: 0.0268\n",
            "Epoch finished\n",
            "Epoch [5/10], Step [1/38], Loss: 0.0441\n",
            "Epoch [5/10], Step [2/38], Loss: 0.0567\n",
            "Epoch [5/10], Step [3/38], Loss: 0.0505\n",
            "Epoch [5/10], Step [4/38], Loss: 0.0229\n",
            "Epoch [5/10], Step [5/38], Loss: 0.0485\n",
            "Epoch [5/10], Step [6/38], Loss: 0.0434\n",
            "Epoch [5/10], Step [7/38], Loss: 0.0969\n",
            "Epoch [5/10], Step [8/38], Loss: 0.0869\n",
            "Epoch [5/10], Step [9/38], Loss: 0.0413\n",
            "Epoch [5/10], Step [10/38], Loss: 0.0468\n",
            "Epoch [5/10], Step [11/38], Loss: 0.0372\n",
            "Epoch [5/10], Step [12/38], Loss: 0.0203\n",
            "Epoch [5/10], Step [13/38], Loss: 0.0622\n",
            "Epoch [5/10], Step [14/38], Loss: 0.0535\n",
            "Epoch [5/10], Step [15/38], Loss: 0.0507\n",
            "Epoch [5/10], Step [16/38], Loss: 0.0503\n",
            "Epoch [5/10], Step [17/38], Loss: 0.0967\n",
            "Epoch [5/10], Step [18/38], Loss: 0.0503\n",
            "Epoch [5/10], Step [19/38], Loss: 0.0432\n",
            "Epoch [5/10], Step [20/38], Loss: 0.0777\n",
            "Epoch [5/10], Step [21/38], Loss: 0.0609\n",
            "Epoch [5/10], Step [22/38], Loss: 0.0320\n",
            "Epoch [5/10], Step [23/38], Loss: 0.0498\n",
            "Epoch [5/10], Step [24/38], Loss: 0.0763\n",
            "Epoch [5/10], Step [25/38], Loss: 0.0425\n",
            "Epoch [5/10], Step [26/38], Loss: 0.1001\n",
            "Epoch [5/10], Step [27/38], Loss: 0.0391\n",
            "Epoch [5/10], Step [28/38], Loss: 0.0372\n",
            "Epoch [5/10], Step [29/38], Loss: 0.0381\n",
            "Epoch [5/10], Step [30/38], Loss: 0.0391\n",
            "Epoch [5/10], Step [31/38], Loss: 0.0616\n",
            "Epoch [5/10], Step [32/38], Loss: 0.0521\n",
            "Epoch [5/10], Step [33/38], Loss: 0.0330\n",
            "Epoch [5/10], Step [34/38], Loss: 0.0698\n",
            "Epoch [5/10], Step [35/38], Loss: 0.0524\n",
            "Epoch [5/10], Step [36/38], Loss: 0.0771\n",
            "Epoch [5/10], Step [37/38], Loss: 0.0775\n",
            "Epoch [5/10], Step [38/38], Loss: 0.0396\n",
            "Epoch finished\n",
            "Epoch [6/10], Step [1/38], Loss: 0.0559\n",
            "Epoch [6/10], Step [2/38], Loss: 0.0311\n",
            "Epoch [6/10], Step [3/38], Loss: 0.0354\n",
            "Epoch [6/10], Step [4/38], Loss: 0.0390\n",
            "Epoch [6/10], Step [5/38], Loss: 0.0511\n",
            "Epoch [6/10], Step [6/38], Loss: 0.0259\n",
            "Epoch [6/10], Step [7/38], Loss: 0.0757\n",
            "Epoch [6/10], Step [8/38], Loss: 0.0733\n",
            "Epoch [6/10], Step [9/38], Loss: 0.0820\n",
            "Epoch [6/10], Step [10/38], Loss: 0.0671\n",
            "Epoch [6/10], Step [11/38], Loss: 0.0444\n",
            "Epoch [6/10], Step [12/38], Loss: 0.0390\n",
            "Epoch [6/10], Step [13/38], Loss: 0.0665\n",
            "Epoch [6/10], Step [14/38], Loss: 0.0376\n",
            "Epoch [6/10], Step [15/38], Loss: 0.0602\n",
            "Epoch [6/10], Step [16/38], Loss: 0.0567\n",
            "Epoch [6/10], Step [17/38], Loss: 0.0676\n",
            "Epoch [6/10], Step [18/38], Loss: 0.0357\n",
            "Epoch [6/10], Step [19/38], Loss: 0.0274\n",
            "Epoch [6/10], Step [20/38], Loss: 0.0687\n",
            "Epoch [6/10], Step [21/38], Loss: 0.0433\n",
            "Epoch [6/10], Step [22/38], Loss: 0.0303\n",
            "Epoch [6/10], Step [23/38], Loss: 0.0298\n",
            "Epoch [6/10], Step [24/38], Loss: 0.0460\n",
            "Epoch [6/10], Step [25/38], Loss: 0.0698\n",
            "Epoch [6/10], Step [26/38], Loss: 0.0798\n",
            "Epoch [6/10], Step [27/38], Loss: 0.0441\n",
            "Epoch [6/10], Step [28/38], Loss: 0.0331\n",
            "Epoch [6/10], Step [29/38], Loss: 0.0447\n",
            "Epoch [6/10], Step [30/38], Loss: 0.0363\n",
            "Epoch [6/10], Step [31/38], Loss: 0.0556\n",
            "Epoch [6/10], Step [32/38], Loss: 0.0453\n",
            "Epoch [6/10], Step [33/38], Loss: 0.0512\n",
            "Epoch [6/10], Step [34/38], Loss: 0.0627\n",
            "Epoch [6/10], Step [35/38], Loss: 0.0294\n",
            "Epoch [6/10], Step [36/38], Loss: 0.0741\n",
            "Epoch [6/10], Step [37/38], Loss: 0.1141\n",
            "Epoch [6/10], Step [38/38], Loss: 0.0271\n",
            "Epoch finished\n",
            "Epoch [7/10], Step [1/38], Loss: 0.0674\n",
            "Epoch [7/10], Step [2/38], Loss: 0.0357\n",
            "Epoch [7/10], Step [3/38], Loss: 0.0447\n",
            "Epoch [7/10], Step [4/38], Loss: 0.0213\n",
            "Epoch [7/10], Step [5/38], Loss: 0.0733\n",
            "Epoch [7/10], Step [6/38], Loss: 0.0369\n",
            "Epoch [7/10], Step [7/38], Loss: 0.0675\n",
            "Epoch [7/10], Step [8/38], Loss: 0.0407\n",
            "Epoch [7/10], Step [9/38], Loss: 0.0554\n",
            "Epoch [7/10], Step [10/38], Loss: 0.0310\n",
            "Epoch [7/10], Step [11/38], Loss: 0.0826\n",
            "Epoch [7/10], Step [12/38], Loss: 0.0139\n",
            "Epoch [7/10], Step [13/38], Loss: 0.0626\n",
            "Epoch [7/10], Step [14/38], Loss: 0.0486\n",
            "Epoch [7/10], Step [15/38], Loss: 0.0329\n",
            "Epoch [7/10], Step [16/38], Loss: 0.0319\n",
            "Epoch [7/10], Step [17/38], Loss: 0.0514\n",
            "Epoch [7/10], Step [18/38], Loss: 0.0313\n",
            "Epoch [7/10], Step [19/38], Loss: 0.0282\n",
            "Epoch [7/10], Step [20/38], Loss: 0.0443\n",
            "Epoch [7/10], Step [21/38], Loss: 0.0470\n",
            "Epoch [7/10], Step [22/38], Loss: 0.0389\n",
            "Epoch [7/10], Step [23/38], Loss: 0.0261\n",
            "Epoch [7/10], Step [24/38], Loss: 0.0426\n",
            "Epoch [7/10], Step [25/38], Loss: 0.0661\n",
            "Epoch [7/10], Step [26/38], Loss: 0.0451\n",
            "Epoch [7/10], Step [27/38], Loss: 0.0746\n",
            "Epoch [7/10], Step [28/38], Loss: 0.0619\n",
            "Epoch [7/10], Step [29/38], Loss: 0.0418\n",
            "Epoch [7/10], Step [30/38], Loss: 0.0300\n",
            "Epoch [7/10], Step [31/38], Loss: 0.0600\n",
            "Epoch [7/10], Step [32/38], Loss: 0.0617\n",
            "Epoch [7/10], Step [33/38], Loss: 0.0432\n",
            "Epoch [7/10], Step [34/38], Loss: 0.0356\n",
            "Epoch [7/10], Step [35/38], Loss: 0.0393\n",
            "Epoch [7/10], Step [36/38], Loss: 0.0539\n",
            "Epoch [7/10], Step [37/38], Loss: 0.0660\n",
            "Epoch [7/10], Step [38/38], Loss: 0.0141\n",
            "Epoch finished\n",
            "Epoch [8/10], Step [1/38], Loss: 0.0231\n",
            "Epoch [8/10], Step [2/38], Loss: 0.0310\n",
            "Epoch [8/10], Step [3/38], Loss: 0.0449\n",
            "Epoch [8/10], Step [4/38], Loss: 0.0449\n",
            "Epoch [8/10], Step [5/38], Loss: 0.0714\n",
            "Epoch [8/10], Step [6/38], Loss: 0.0337\n",
            "Epoch [8/10], Step [7/38], Loss: 0.0215\n",
            "Epoch [8/10], Step [8/38], Loss: 0.0472\n",
            "Epoch [8/10], Step [9/38], Loss: 0.0130\n",
            "Epoch [8/10], Step [10/38], Loss: 0.0287\n",
            "Epoch [8/10], Step [11/38], Loss: 0.0326\n",
            "Epoch [8/10], Step [12/38], Loss: 0.0157\n",
            "Epoch [8/10], Step [13/38], Loss: 0.0497\n",
            "Epoch [8/10], Step [14/38], Loss: 0.0510\n",
            "Epoch [8/10], Step [15/38], Loss: 0.0117\n",
            "Epoch [8/10], Step [16/38], Loss: 0.0356\n",
            "Epoch [8/10], Step [17/38], Loss: 0.0700\n",
            "Epoch [8/10], Step [18/38], Loss: 0.0457\n",
            "Epoch [8/10], Step [19/38], Loss: 0.0475\n",
            "Epoch [8/10], Step [20/38], Loss: 0.0535\n",
            "Epoch [8/10], Step [21/38], Loss: 0.0511\n",
            "Epoch [8/10], Step [22/38], Loss: 0.0226\n",
            "Epoch [8/10], Step [23/38], Loss: 0.0424\n",
            "Epoch [8/10], Step [24/38], Loss: 0.0442\n",
            "Epoch [8/10], Step [25/38], Loss: 0.0246\n",
            "Epoch [8/10], Step [26/38], Loss: 0.0423\n",
            "Epoch [8/10], Step [27/38], Loss: 0.0679\n",
            "Epoch [8/10], Step [28/38], Loss: 0.0480\n",
            "Epoch [8/10], Step [29/38], Loss: 0.0501\n",
            "Epoch [8/10], Step [30/38], Loss: 0.0284\n",
            "Epoch [8/10], Step [31/38], Loss: 0.0515\n",
            "Epoch [8/10], Step [32/38], Loss: 0.0263\n",
            "Epoch [8/10], Step [33/38], Loss: 0.0602\n",
            "Epoch [8/10], Step [34/38], Loss: 0.0336\n",
            "Epoch [8/10], Step [35/38], Loss: 0.0540\n",
            "Epoch [8/10], Step [36/38], Loss: 0.0468\n",
            "Epoch [8/10], Step [37/38], Loss: 0.0828\n",
            "Epoch [8/10], Step [38/38], Loss: 0.0295\n",
            "Epoch finished\n",
            "Epoch [9/10], Step [1/38], Loss: 0.0388\n",
            "Epoch [9/10], Step [2/38], Loss: 0.0277\n",
            "Epoch [9/10], Step [3/38], Loss: 0.0592\n",
            "Epoch [9/10], Step [4/38], Loss: 0.0164\n",
            "Epoch [9/10], Step [5/38], Loss: 0.0432\n",
            "Epoch [9/10], Step [6/38], Loss: 0.0158\n",
            "Epoch [9/10], Step [7/38], Loss: 0.0297\n",
            "Epoch [9/10], Step [8/38], Loss: 0.0568\n",
            "Epoch [9/10], Step [9/38], Loss: 0.0541\n",
            "Epoch [9/10], Step [10/38], Loss: 0.0468\n",
            "Epoch [9/10], Step [11/38], Loss: 0.0239\n",
            "Epoch [9/10], Step [12/38], Loss: 0.0174\n",
            "Epoch [9/10], Step [13/38], Loss: 0.0281\n",
            "Epoch [9/10], Step [14/38], Loss: 0.0771\n",
            "Epoch [9/10], Step [15/38], Loss: 0.0329\n",
            "Epoch [9/10], Step [16/38], Loss: 0.0288\n",
            "Epoch [9/10], Step [17/38], Loss: 0.0583\n",
            "Epoch [9/10], Step [18/38], Loss: 0.0446\n",
            "Epoch [9/10], Step [19/38], Loss: 0.0448\n",
            "Epoch [9/10], Step [20/38], Loss: 0.0253\n",
            "Epoch [9/10], Step [21/38], Loss: 0.0222\n",
            "Epoch [9/10], Step [22/38], Loss: 0.0537\n",
            "Epoch [9/10], Step [23/38], Loss: 0.0238\n",
            "Epoch [9/10], Step [24/38], Loss: 0.0609\n",
            "Epoch [9/10], Step [25/38], Loss: 0.0423\n",
            "Epoch [9/10], Step [26/38], Loss: 0.0801\n",
            "Epoch [9/10], Step [27/38], Loss: 0.0486\n",
            "Epoch [9/10], Step [28/38], Loss: 0.0430\n",
            "Epoch [9/10], Step [29/38], Loss: 0.0170\n",
            "Epoch [9/10], Step [30/38], Loss: 0.0188\n",
            "Epoch [9/10], Step [31/38], Loss: 0.0472\n",
            "Epoch [9/10], Step [32/38], Loss: 0.0420\n",
            "Epoch [9/10], Step [33/38], Loss: 0.0227\n",
            "Epoch [9/10], Step [34/38], Loss: 0.0159\n",
            "Epoch [9/10], Step [35/38], Loss: 0.0746\n",
            "Epoch [9/10], Step [36/38], Loss: 0.0704\n",
            "Epoch [9/10], Step [37/38], Loss: 0.0618\n",
            "Epoch [9/10], Step [38/38], Loss: 0.0213\n",
            "Epoch finished\n",
            "Epoch [10/10], Step [1/38], Loss: 0.0283\n",
            "Epoch [10/10], Step [2/38], Loss: 0.0216\n",
            "Epoch [10/10], Step [3/38], Loss: 0.0614\n",
            "Epoch [10/10], Step [4/38], Loss: 0.0230\n",
            "Epoch [10/10], Step [5/38], Loss: 0.0274\n",
            "Epoch [10/10], Step [6/38], Loss: 0.0453\n",
            "Epoch [10/10], Step [7/38], Loss: 0.0197\n",
            "Epoch [10/10], Step [8/38], Loss: 0.0163\n",
            "Epoch [10/10], Step [9/38], Loss: 0.0172\n",
            "Epoch [10/10], Step [10/38], Loss: 0.0174\n",
            "Epoch [10/10], Step [11/38], Loss: 0.0173\n",
            "Epoch [10/10], Step [12/38], Loss: 0.0173\n",
            "Epoch [10/10], Step [13/38], Loss: 0.0451\n",
            "Epoch [10/10], Step [14/38], Loss: 0.0507\n",
            "Epoch [10/10], Step [15/38], Loss: 0.0345\n",
            "Epoch [10/10], Step [16/38], Loss: 0.0228\n",
            "Epoch [10/10], Step [17/38], Loss: 0.0775\n",
            "Epoch [10/10], Step [18/38], Loss: 0.0306\n",
            "Epoch [10/10], Step [19/38], Loss: 0.0348\n",
            "Epoch [10/10], Step [20/38], Loss: 0.0819\n",
            "Epoch [10/10], Step [21/38], Loss: 0.0128\n",
            "Epoch [10/10], Step [22/38], Loss: 0.0483\n",
            "Epoch [10/10], Step [23/38], Loss: 0.0120\n",
            "Epoch [10/10], Step [24/38], Loss: 0.0639\n",
            "Epoch [10/10], Step [25/38], Loss: 0.0183\n",
            "Epoch [10/10], Step [26/38], Loss: 0.0426\n",
            "Epoch [10/10], Step [27/38], Loss: 0.0521\n",
            "Epoch [10/10], Step [28/38], Loss: 0.0392\n",
            "Epoch [10/10], Step [29/38], Loss: 0.0375\n",
            "Epoch [10/10], Step [30/38], Loss: 0.0199\n",
            "Epoch [10/10], Step [31/38], Loss: 0.0724\n",
            "Epoch [10/10], Step [32/38], Loss: 0.0382\n",
            "Epoch [10/10], Step [33/38], Loss: 0.0326\n",
            "Epoch [10/10], Step [34/38], Loss: 0.0603\n",
            "Epoch [10/10], Step [35/38], Loss: 0.0608\n",
            "Epoch [10/10], Step [36/38], Loss: 0.0436\n",
            "Epoch [10/10], Step [37/38], Loss: 0.0299\n",
            "Epoch [10/10], Step [38/38], Loss: 0.0132\n",
            "Epoch finished\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = vgg(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "metadata": {
        "id": "YfdbdbajvmYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cf1480-e216-4914-8d48-38d6f8832e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 98 %\n"
          ]
        }
      ]
    }
  ]
}